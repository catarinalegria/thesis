{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /cfs/home/u021320/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = []\n",
    "\n",
    "for i in range(1,11):\n",
    "\n",
    "    # load season i\n",
    "    if i<10:\n",
    "        json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_0'+str(i)+'.json'\n",
    "    else:\n",
    "        json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_10.json'\n",
    "    \n",
    "    r = requests.get(json_file)\n",
    "    \n",
    "    seasons.append(json.loads(r.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes:  236\n",
      "Number of scenes:  3107\n",
      "Number of utterances:  61310\n",
      "Number of speakers:  701\n",
      "Average number of tokens per utterance:  10.159745555374327\n",
      "Average number of utterances per scene:  19.732861280978437\n"
     ]
    }
   ],
   "source": [
    "#for statistical purposes\n",
    "\n",
    "utterances = []\n",
    "episodes = []\n",
    "scenes = []\n",
    "speakers = []\n",
    "\n",
    "utterances_lines = []\n",
    "\n",
    "\n",
    "for season in seasons:\n",
    "    for episode in season['episodes']:\n",
    "        episodes.append(episode['episode_id'])\n",
    "        for scene in episode['scenes']:\n",
    "            scenes.append(scene['scene_id'])\n",
    "            for utterance in scene['utterances']:\n",
    "                #dont want non-speech\n",
    "                if utterance['transcript'] != \"\":\n",
    "                    utterances.append(utterance['utterance_id'])\n",
    "                    utterances_lines.append(utterance['transcript'])\n",
    "                    for speaker in utterance['speakers']:\n",
    "                        if speaker not in speakers:\n",
    "                            speakers.append(utterance['speakers'][0])\n",
    "\n",
    "print(\"Number of episodes: \", len(episodes))\n",
    "print(\"Number of scenes: \", len(scenes))\n",
    "print(\"Number of utterances: \", len(utterances))\n",
    "print(\"Number of speakers: \", len(speakers))\n",
    "print(\"Average number of tokens per utterance: \", sum([len(utterance.split()) for utterance in utterances_lines])/len(utterances_lines))\n",
    "print(\"Average number of utterances per scene: \", len(utterances)/len(scenes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocalurary statistics by speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monica Geller\n",
      "vocabulary size:  5616\n",
      "Joey Tribbiani\n",
      "vocabulary size:  5949\n",
      "Chandler Bing\n",
      "vocabulary size:  6445\n",
      "Phoebe Buffay\n",
      "vocabulary size:  5989\n",
      "Ross Geller\n",
      "vocabulary size:  6550\n",
      "Rachel Green\n",
      "vocabulary size:  5837\n",
      "stop words ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monica Geller\n",
      "Top 10 words without stop-words:  [('oh', 949), ('im', 708), ('okay', 677), ('dont', 577), ('know', 552), ('well', 475), ('right', 448), ('gonna', 447), ('youre', 436), ('hey', 423)]\n",
      "Joey Tribbiani\n",
      "Top 10 words without stop-words:  [('hey', 999), ('yeah', 936), ('oh', 809), ('im', 805), ('know', 670), ('right', 665), ('dont', 595), ('okay', 555), ('well', 537), ('like', 529)]\n",
      "Chandler Bing\n",
      "Top 10 words without stop-words:  [('im', 801), ('oh', 766), ('okay', 708), ('well', 655), ('know', 613), ('dont', 604), ('yeah', 559), ('hey', 512), ('right', 485), ('get', 437)]\n",
      "Phoebe Buffay\n",
      "Top 10 words without stop-words:  [('oh', 1442), ('okay', 816), ('know', 802), ('yeah', 799), ('im', 742), ('well', 575), ('dont', 562), ('like', 525), ('hey', 465), ('right', 406)]\n",
      "Ross Geller\n",
      "Top 10 words without stop-words:  [('im', 941), ('oh', 898), ('okay', 836), ('yeah', 828), ('know', 746), ('hey', 741), ('uh', 678), ('dont', 661), ('well', 633), ('right', 516)]\n",
      "Rachel Green\n",
      "Top 10 words without stop-words:  [('oh', 1872), ('im', 990), ('know', 863), ('well', 840), ('okay', 829), ('yeah', 798), ('dont', 703), ('right', 566), ('gonna', 559), ('ross', 523)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "characters = ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green']\n",
    "\n",
    "for speaker in characters:\n",
    "    utterances = []\n",
    "    for season in seasons:\n",
    "        for episode in season['episodes']:\n",
    "            for scene in episode['scenes']:\n",
    "                for utterance in scene['utterances']:\n",
    "                    if speaker in utterance['speakers'] and utterance['transcript']!=\"\":\n",
    "                        for word in utterance['transcript'].translate(str.maketrans('', '', string.punctuation)).lower().split():\n",
    "                            utterances.append(word)\n",
    "    print(speaker)\n",
    "    print(\"vocabulary size: \", len(Counter(utterances)))\n",
    "\n",
    "print(\"stop words\", stopwords.words('english'))\n",
    "\n",
    "for speaker in characters:\n",
    "    utterances = []\n",
    "    for season in seasons:\n",
    "        for episode in season['episodes']:\n",
    "            for scene in episode['scenes']:\n",
    "                for utterance in scene['utterances']:\n",
    "                    if speaker in utterance['speakers'] and utterance['transcript']!=\"\":\n",
    "                        for word in utterance['transcript'].translate(str.maketrans('', '', string.punctuation)).lower().split():\n",
    "                            # we do not want stopwords for 10 words\n",
    "                            if word not in stopwords.words('english'):\n",
    "                                utterances.append(word)\n",
    "    print(speaker)\n",
    "    print(\"Top 10 words without stop-words: \", Counter(utterances).most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set 1: season 8\n",
    "#train set 2: first 7 seasons\n",
    "\n",
    "test_set = []\n",
    "\n",
    "for episode in seasons[7]['episodes']:\n",
    "    for scene in episode['scenes']:\n",
    "        for utterance in scene['utterances']:\n",
    "            if utterance['speakers'] != []:\n",
    "                test_set.append(utterance)\n",
    "\n",
    "\n",
    "train_set = []\n",
    "\n",
    "for season in seasons:\n",
    "    if season['season_id'] in ['s01', 's02', 's03', 's04', 's05', 's06', 's07']:\n",
    "        for episode in season['episodes']:\n",
    "            for scene in episode['scenes']:\n",
    "                for utterance in scene['utterances']:\n",
    "                    if utterance['speakers'] != [] and utterance not in test_set:\n",
    "                        train_set.append(utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes:  194\n",
      "Number of scenes:  2579\n",
      "Number of utterances:  49740\n",
      "Number of speakers:  596\n",
      "Average number of tokens per utterance:  10.07227583433856\n",
      "Average number of utterances per scene:  19.2865451725475\n",
      "Monica:  0.13772106109324758\n",
      "Chandler:  0.13818327974276529\n",
      "Ross:  0.14887459807073955\n",
      "Joey:  0.132516077170418\n",
      "Phoebe:  0.12067926045016077\n",
      "Rachel:  0.1534967845659164\n",
      "Others:  0.1685289389067524\n"
     ]
    }
   ],
   "source": [
    "utterances = []\n",
    "episodes = []\n",
    "scenes = []\n",
    "speakers = []\n",
    "\n",
    "utterances_lines = []\n",
    "\n",
    "set1 = test_set + train_set\n",
    "\n",
    "for utterance in set1:\n",
    "    if utterance['transcript'] != \"\":\n",
    "        utterances.append(utterance['utterance_id'])\n",
    "        utterances_lines.append(utterance['transcript'])\n",
    "\n",
    "        #regex to remove episode from utterance id\n",
    "        episode = utterance['utterance_id'][0:7]\n",
    "        scene = utterance['utterance_id'][0:11]\n",
    "\n",
    "        if episode not in episodes:\n",
    "            episodes.append(episode)\n",
    "\n",
    "        if scene not in scenes:\n",
    "            scenes.append(scene)\n",
    "\n",
    "        for speaker in utterance['speakers']:\n",
    "            if speaker not in speakers:\n",
    "                speakers.append(utterance['speakers'][0])\n",
    "\n",
    "print(\"Number of episodes: \", len(episodes))\n",
    "print(\"Number of scenes: \", len(scenes))\n",
    "print(\"Number of utterances: \", len(utterances))\n",
    "print(\"Number of speakers: \", len(speakers))\n",
    "print(\"Average number of tokens per utterance: \", sum([len(utterance.split()) for utterance in utterances_lines])/len(utterances_lines))\n",
    "print(\"Average number of utterances per scene: \", len(utterances)/len(scenes))\n",
    "\n",
    "\n",
    "#lets count utterances per speaker\n",
    "monica = []\n",
    "chandler = []\n",
    "ross = []\n",
    "joey = []\n",
    "phoebe = []\n",
    "rachel = []\n",
    "others = []\n",
    "\n",
    "\n",
    "\n",
    "for utterance in set1:\n",
    "    if utterance['speakers'] == ['Monica Geller']:\n",
    "        monica.append(utterance)\n",
    "    elif utterance['speakers'] == ['Chandler Bing']:\n",
    "        chandler.append(utterance)\n",
    "    elif utterance['speakers'] == ['Ross Geller']:\n",
    "        ross.append(utterance)\n",
    "    elif utterance['speakers'] == ['Joey Tribbiani']:\n",
    "        joey.append(utterance)\n",
    "    elif utterance['speakers'] == ['Phoebe Buffay']:\n",
    "        phoebe.append(utterance)\n",
    "    elif utterance['speakers'] == ['Rachel Green']:\n",
    "        rachel.append(utterance)\n",
    "    else:\n",
    "        others.append(utterance)\n",
    "\n",
    "#lets get the percentages of each speaker\n",
    "print(\"Monica: \", len(monica)/len(set1))\n",
    "print(\"Chandler: \", len(chandler)/len(set1))\n",
    "print(\"Ross: \", len(ross)/len(set1))\n",
    "print(\"Joey: \", len(joey)/len(set1))\n",
    "print(\"Phoebe: \", len(phoebe)/len(set1))\n",
    "print(\"Rachel: \", len(rachel)/len(set1))\n",
    "print(\"Others: \", len(others)/len(set1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sets/test_set1.json\", \"w\") as outfile:\n",
    "    json.dump(test_set, outfile)\n",
    "\n",
    "\n",
    "with open(\"sets/train_set1.json\", \"w\") as outfile:\n",
    "    json.dump(train_set, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set 2: 10% of utterances from each season\n",
    "\n",
    "all_seasons = []\n",
    "for season in seasons:\n",
    "    utterances = []\n",
    "    selection = []\n",
    "    for episode in season['episodes']:\n",
    "        for scene in episode['scenes']:\n",
    "            for utterance in scene['utterances']:\n",
    "                if utterance['speakers'] != []:\n",
    "                    utterances.append(utterance)\n",
    "\n",
    "    num_elements = len(utterances)\n",
    "    num_elements_to_extract = int(num_elements * 0.1)  # Calculate 10% of the total elements\n",
    "    selection = random.sample(utterances, num_elements_to_extract)\n",
    "\n",
    "    all_seasons.append(selection)\n",
    "\n",
    "\n",
    "test_set = []\n",
    "for season in all_seasons:\n",
    "    for utterance in season:\n",
    "        test_set.append(utterance)\n",
    "\n",
    "train_set = []\n",
    "for season in seasons:\n",
    "    for episode in season['episodes']:\n",
    "        for scene in episode['scenes']:\n",
    "            for utterance in scene['utterances']:\n",
    "                if utterance['speakers'] != [] and utterance not in test_set:\n",
    "                    train_set.append(utterance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes:  236\n",
      "Number of scenes:  3099\n",
      "Number of utterances:  61308\n",
      "Number of speakers:  701\n",
      "Average number of tokens per utterance:  10.159881255301103\n",
      "Average number of utterances per scene:  19.783155856727976\n",
      "Monica:  0.13763083243666244\n",
      "Chandler:  0.13800580390622452\n",
      "Ross:  0.1493038573152043\n",
      "Joey:  0.13331050898301217\n",
      "Phoebe:  0.12233851772147772\n",
      "Rachel:  0.1518634451726499\n",
      "Others:  0.16754703446476898\n"
     ]
    }
   ],
   "source": [
    "utterances = []\n",
    "episodes = []\n",
    "scenes = []\n",
    "speakers = []\n",
    "\n",
    "utterances_lines = []\n",
    "\n",
    "set2 = test_set + train_set\n",
    "\n",
    "for utterance in set2:\n",
    "    if utterance['transcript'] != \"\":\n",
    "        utterances.append(utterance['utterance_id'])\n",
    "        utterances_lines.append(utterance['transcript'])\n",
    "\n",
    "        #regex to remove episode from utterance id\n",
    "        episode = utterance['utterance_id'][0:7]\n",
    "        scene = utterance['utterance_id'][0:11]\n",
    "\n",
    "        if episode not in episodes:\n",
    "            episodes.append(episode)\n",
    "\n",
    "        if scene not in scenes:\n",
    "            scenes.append(scene)\n",
    "\n",
    "        for speaker in utterance['speakers']:\n",
    "            if speaker not in speakers:\n",
    "                speakers.append(utterance['speakers'][0])\n",
    "\n",
    "print(\"Number of episodes: \", len(episodes))\n",
    "print(\"Number of scenes: \", len(scenes))\n",
    "print(\"Number of utterances: \", len(utterances))\n",
    "print(\"Number of speakers: \", len(speakers))\n",
    "print(\"Average number of tokens per utterance: \", sum([len(utterance.split()) for utterance in utterances_lines])/len(utterances_lines))\n",
    "print(\"Average number of utterances per scene: \", len(utterances)/len(scenes))\n",
    "\n",
    "\n",
    "monica = []\n",
    "chandler = []\n",
    "ross = []\n",
    "joey = []\n",
    "phoebe = []\n",
    "rachel = []\n",
    "others = []\n",
    "\n",
    "for utterance in set2:\n",
    "    if utterance['speakers'] == ['Monica Geller']:\n",
    "        monica.append(utterance)\n",
    "    elif utterance['speakers'] == ['Chandler Bing']:\n",
    "        chandler.append(utterance)\n",
    "    elif utterance['speakers'] == ['Ross Geller']:\n",
    "        ross.append(utterance)\n",
    "    elif utterance['speakers'] == ['Joey Tribbiani']:\n",
    "        joey.append(utterance)\n",
    "    elif utterance['speakers'] == ['Phoebe Buffay']:\n",
    "        phoebe.append(utterance)\n",
    "    elif utterance['speakers'] == ['Rachel Green']:\n",
    "        rachel.append(utterance)\n",
    "    else:\n",
    "        others.append(utterance)\n",
    "\n",
    "#lets get the percentages of each speaker\n",
    "print(\"Monica: \", len(monica)/len(set2))\n",
    "print(\"Chandler: \", len(chandler)/len(set2))\n",
    "print(\"Ross: \", len(ross)/len(set2))\n",
    "print(\"Joey: \", len(joey)/len(set2))\n",
    "print(\"Phoebe: \", len(phoebe)/len(set2))\n",
    "print(\"Rachel: \", len(rachel)/len(set2))\n",
    "print(\"Others: \", len(others)/len(set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sets/test_set2.json\", \"w\") as outfile:\n",
    "    json.dump(test_set, outfile)\n",
    "\n",
    "\n",
    "with open(\"sets/train_set2.json\", \"w\") as outfile:\n",
    "    json.dump(train_set, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
