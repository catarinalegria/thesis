{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green', 'others']\n",
    "\n",
    "#test set 1 = season 8\n",
    "\n",
    "with open('sets/test_set1.json') as f:\n",
    "    test_set1 = json.load(f)\n",
    "\n",
    "with open('sets/train_set1.json') as f:\n",
    "    train_set1 = json.load(f)\n",
    "\n",
    "#CHANGE HERE\n",
    "test_set = test_set1\n",
    "train_set = train_set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_calculator(lines):    \n",
    "    # Compute the term frequency of each word in each sentence\n",
    "    tf = []\n",
    "    \n",
    "    for sentence in lines:\n",
    "        tf_sentence = {}\n",
    "        for word in sentence.split():\n",
    "            tf_sentence[word] = tf_sentence.get(word, 0) + 1\n",
    "        tf.append(tf_sentence)\n",
    "\n",
    "    # Compute the inverse document frequency of each word\n",
    "    idf = {}\n",
    "    for sentence in lines:\n",
    "        for word in sentence.split():\n",
    "            idf[word] = idf.get(word, 0) + 1\n",
    "    for word in idf:\n",
    "        idf[word] = math.log(len(lines) / idf[word])\n",
    "\n",
    "    # Compute the tf-idf of each sentence\n",
    "    tf_idf = []\n",
    "    for i, sentence in enumerate(lines):\n",
    "        tf_idf_sentence = 0\n",
    "        for word in tf[i]:\n",
    "            tf_idf_sentence += tf[i][word] * idf[word]\n",
    "        tf_idf.append(tf_idf_sentence)\n",
    "    \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "\n",
    "previous = 2\n",
    "next = 2\n",
    "\n",
    "for character in characters:\n",
    "\n",
    "    lines_withcontext = []\n",
    "    lines = []\n",
    "\n",
    "    for idx, utterance in enumerate(train_set):\n",
    "        if utterance['speakers'] != [] and character in utterance['speakers'] and idx not in [0, 1] and idx != len(train_set)- 1 and idx != len(train_set)- 2:\n",
    "            lines_withcontext.append([utterance['transcript'], train_set[idx-2]['transcript'],train_set[idx-1]['transcript'], train_set[idx+1]['transcript'], train_set[idx+2]['transcript']])\n",
    "            lines.append(utterance['transcript'])\n",
    "        elif idx ==0:\n",
    "            lines_withcontext.append([utterance['transcript'], \"\", \"\", train_set[idx+1]['transcript'], train_set[idx+2]['transcript']])\n",
    "            lines.append(utterance['transcript'])\n",
    "        elif idx ==1:\n",
    "            lines_withcontext.append([utterance['transcript'], \"\", train_set[idx-1]['transcript'], train_set[idx+1]['transcript'],  train_set[idx+2]['transcript']])\n",
    "            lines.append(utterance['transcript'])\n",
    "        elif idx == len(train_set)-1:\n",
    "            lines_withcontext.append([utterance['transcript'], train_set[idx-2]['transcript'] ,train_set[idx-1]['transcript'], \"\", \"\"])\n",
    "            lines.append(utterance['transcript'])\n",
    "        elif idx == len(train_set)-2:\n",
    "            lines_withcontext.append([utterance['transcript'], train_set[idx-2]['transcript'] ,train_set[idx-1]['transcript'], train_set[idx+1]['transcript'], \"\"])\n",
    "            lines.append(utterance['transcript'])\n",
    "    \n",
    "\n",
    "    tf_idf = tf_idf_calculator(lines)\n",
    "\n",
    "    final = []\n",
    "\n",
    "    #format: final = [...,[sentence, tf-idf score],...]\n",
    "    for i,sentence in enumerate(lines):\n",
    "        cell = []\n",
    "        cell= [tf_idf[i], sentence]\n",
    "        final.append(cell)\n",
    "    \n",
    "    # lower -> higher     \n",
    "    sorted_list = sorted(final)\n",
    "\n",
    "    l = []\n",
    "\n",
    "    #get only the strings\n",
    "    for elem in sorted_list:\n",
    "        l.append(elem[1])\n",
    "\n",
    "    #remove duplicates\n",
    "    l = list( dict.fromkeys(l) )\n",
    "\n",
    "    #change x for number of lines \n",
    "    values = [5,10,20,50,100,150,200,500,1000,2000,5000, 6000]\n",
    "\n",
    "    for x in values:\n",
    "\n",
    "        higher = l[-x:]\n",
    "        lower = l[:x]\n",
    "\n",
    "        f = open(\"embeddingscontext1/\" + character + str(x)+ \".txt\", \"w\")\n",
    "        for line in higher:\n",
    "            index = lines.index(line)\n",
    "\n",
    "            for utterance in lines_withcontext[index]:\n",
    "                f.write(utterance +\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "\n",
    "################################################################\n",
    "#others\n",
    "\n",
    "lines_withcontext = []\n",
    "lines = []\n",
    "\n",
    "for idx, utterance in enumerate(train_set):\n",
    "    if utterance['speakers'] != [] and utterance['speakers'][0] not in characters and idx not in [0, 1] and idx != len(train_set)- 1 and idx != len(train_set)- 2:\n",
    "        lines_withcontext.append([utterance['transcript'], train_set[idx-2]['transcript'],train_set[idx-1]['transcript'], train_set[idx+1]['transcript'], train_set[idx+2]['transcript']])\n",
    "        lines.append(utterance['transcript'])\n",
    "    elif idx ==0:\n",
    "        lines_withcontext.append([utterance['transcript'], \"\", \"\", train_set[idx+1]['transcript'], train_set[idx+2]['transcript']])\n",
    "        lines.append(utterance['transcript'])\n",
    "    elif idx ==1:\n",
    "        lines_withcontext.append([utterance['transcript'], \"\", train_set[idx-1]['transcript'], train_set[idx+1]['transcript'], train_set[idx+2]['transcript']])\n",
    "        lines.append(utterance['transcript'])\n",
    "    elif idx == len(train_set)-1:\n",
    "        lines_withcontext.append([utterance['transcript'], train_set[idx-2]['transcript'] ,train_set[idx-1]['transcript'], \"\", \"\"])\n",
    "        lines.append(utterance['transcript'])\n",
    "    elif idx == len(train_set)-2:\n",
    "        lines_withcontext.append([utterance['transcript'], train_set[idx-2]['transcript'] ,train_set[idx-1]['transcript'], train_set[idx+1]['transcript'], \"\"])\n",
    "        lines.append(utterance['transcript'])\n",
    "    \n",
    "    \n",
    "tf_idf = tf_idf_calculator(lines)\n",
    "\n",
    "final = []\n",
    "\n",
    "#format: final = [...,[sentence, tf-idf score],...]\n",
    "for i,sentence in enumerate(lines):\n",
    "    cell = []\n",
    "    cell= [tf_idf[i], sentence]\n",
    "    final.append(cell)\n",
    "    \n",
    "# lower -> higher     \n",
    "sorted_list = sorted(final)\n",
    "\n",
    "l = []\n",
    "\n",
    "#get only the strings\n",
    "for elem in sorted_list:\n",
    "    l.append(elem[1])\n",
    "\n",
    "#remove duplicates\n",
    "l = list( dict.fromkeys(l) )\n",
    "\n",
    "#change x for number of lines \n",
    "values = [5,10,20,50,100,150,200,500,1000,2000,5000, 6000]\n",
    "\n",
    "for x in values:\n",
    "\n",
    "    higher = l[-x:]\n",
    "    lower = l[:x]\n",
    "\n",
    "    f = open(\"embeddingscontext1/\" + \"others\" + str(x)+ \".txt\", \"w\")\n",
    "    for line in higher:\n",
    "        index = lines.index(line)\n",
    "\n",
    "        for utterance in lines_withcontext[index]:\n",
    "            f.write(utterance +\"\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "\n",
    "\n",
    "for character in characters:\n",
    "\n",
    "    lines_withcontext = []\n",
    "    lines = []\n",
    "\n",
    "    for idx, utterance in enumerate(train_set):\n",
    "        if utterance['speakers'] != [] and character in utterance['speakers'] and idx !=0:\n",
    "            lines_withcontext.append([utterance['transcript'], train_set[idx-1]['transcript']])\n",
    "            lines.append(utterance['transcript'])\n",
    "        elif idx ==0:\n",
    "            lines_withcontext.append([utterance['transcript'], \"\"])\n",
    "            lines.append(utterance['transcript'])\n",
    "    \n",
    "\n",
    "    tf_idf = tf_idf_calculator(lines)\n",
    "\n",
    "    final = []\n",
    "\n",
    "    #format: final = [...,[sentence, tf-idf score],...]\n",
    "    for i,sentence in enumerate(lines):\n",
    "        cell = []\n",
    "        cell= [tf_idf[i], sentence]\n",
    "        final.append(cell)\n",
    "    \n",
    "    # lower -> higher     \n",
    "    sorted_list = sorted(final)\n",
    "\n",
    "    l = []\n",
    "\n",
    "    #get only the strings\n",
    "    for elem in sorted_list:\n",
    "        l.append(elem[1])\n",
    "\n",
    "    #remove duplicates\n",
    "    l = list( dict.fromkeys(l) )\n",
    "\n",
    "    #change x for number of lines \n",
    "    values = [5,10,20,50,100,150,200,500,1000,2000,5000, 6000]\n",
    "\n",
    "    for x in values:\n",
    "\n",
    "        higher = l[-x:]\n",
    "        lower = l[:x]\n",
    "\n",
    "        #substitute embeddings1/2\n",
    "\n",
    "        f = open(\"embeddingscontext2/\" + character + str(x)+ \".txt\", \"w\")\n",
    "        for line in higher:\n",
    "            index = lines.index(line)\n",
    "\n",
    "            for utterance in lines_withcontext[index]:\n",
    "                f.write(utterance +\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "\n",
    "################################################################\n",
    "#others\n",
    "\n",
    "lines_withcontext = []\n",
    "lines = []\n",
    "\n",
    "for idx, utterance in enumerate(train_set):\n",
    "    if utterance['speakers'] != [] and utterance['speakers'][0] not in characters and idx !=0:\n",
    "        lines_withcontext.append([utterance['transcript'], train_set[idx-1]['transcript']])\n",
    "        lines.append(utterance['transcript'])\n",
    "    elif idx ==0:\n",
    "        lines_withcontext.append([utterance['transcript'], \"\"])\n",
    "        lines.append(utterance['transcript'])\n",
    "    \n",
    "    \n",
    "tf_idf = tf_idf_calculator(lines)\n",
    "\n",
    "final = []\n",
    "\n",
    "#format: final = [...,[sentence, tf-idf score],...]\n",
    "for i,sentence in enumerate(lines):\n",
    "    cell = []\n",
    "    cell= [tf_idf[i], sentence]\n",
    "    final.append(cell)\n",
    "    \n",
    "# lower -> higher     \n",
    "sorted_list = sorted(final)\n",
    "\n",
    "l = []\n",
    "\n",
    "#get only the strings\n",
    "for elem in sorted_list:\n",
    "    l.append(elem[1])\n",
    "\n",
    "#remove duplicates\n",
    "l = list( dict.fromkeys(l) )\n",
    "\n",
    "#change x for number of lines \n",
    "values = [5,10,20,50,100,150,200,500,1000,2000,5000, 6000]\n",
    "\n",
    "for x in values:\n",
    "\n",
    "    higher = l[-x:]\n",
    "    lower = l[:x]\n",
    "\n",
    "    #substitute embeddings1/2\n",
    "\n",
    "    f = open(\"embeddingscontext2/\" + \"others\" + str(x)+ \".txt\", \"w\")\n",
    "    for line in higher:\n",
    "        index = lines.index(line)\n",
    "\n",
    "        for utterance in lines_withcontext[index]:\n",
    "            f.write(utterance +\"\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def compute_embedding(previous_sentence,current_sentence):   \n",
    "    \n",
    "    embedding_previous = model.encode(previous_sentence)\n",
    "    embedding_current = model.encode(current_sentence)\n",
    "\n",
    "    #max\n",
    "    #embedding = np.maximum(embedding_previous, embedding_current)\n",
    "\n",
    "    #min\n",
    "    #embedding = np.minimum(embedding_previous, embedding_current)\n",
    "\n",
    "    #concatenate\n",
    "    embedding = np.concatenate((embedding_previous, embedding_current), axis=0)\n",
    "\n",
    "    # add\n",
    "    #embedding = embedding_previous + embedding_current\n",
    "\n",
    "    # mean\n",
    "    #embedding = np.mean( np.array([ embedding_previous, embedding_current ]), axis=0 )\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding2(current_sentence, previous2_sentence, previous_sentence, next_sentence, next2_sentence):\n",
    "    \n",
    "    embedding_previous2 = model.encode(previous2_sentence)\n",
    "    embedding_previous = model.encode(previous_sentence)\n",
    "    embedding_current = model.encode(current_sentence)\n",
    "    embedding_next = model.encode(next_sentence)\n",
    "    embedding_next2 = model.encode(next2_sentence)\n",
    "\n",
    "\n",
    "    #concatenate\n",
    "    embedding = np.concatenate(( embedding_previous2,embedding_previous, embedding_current, embedding_next, embedding_next2), axis=0)\n",
    "\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for character in characters:\n",
    "    f = open(\"embeddingscontext1/\" + character +\"5000\"+ \".txt\", \"r\")\n",
    "    lines = f.readlines()\n",
    "    sentences.append(lines)\n",
    "\n",
    "'''\n",
    "i=0\n",
    "embeddings = {}\n",
    "for character in characters:\n",
    "    embeddings[character] = []\n",
    "    for idx, utterance in enumerate(sentences[i]):\n",
    "        if (idx % 2)==0:\n",
    "            embeddings[character].append(compute_embedding(sentences[i][idx+1], utterance))\n",
    "\n",
    "    i+=1\n",
    "'''\n",
    "\n",
    "i=0\n",
    "embeddings = {}\n",
    "for character in characters:\n",
    "    embeddings[character] = []\n",
    "    for idx in range(0, len(sentences[i]), 5):\n",
    "        #current, previous, next\n",
    "        embeddings[character].append(compute_embedding2(sentences[i][idx], sentences[i][idx-2],sentences[i][idx-1] ,sentences[i][idx+1], sentences[i][idx+2]))\n",
    "\n",
    "    i+=1\n",
    "\n",
    "#print(embeddings)\n",
    "\n",
    "for character in characters:\n",
    "\n",
    "    sum = 0\n",
    "    for embedding in embeddings[character]:\n",
    "        sum += embedding\n",
    "\n",
    "    \n",
    "    embeddings[character] = sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(vector):\n",
    "\n",
    "    angles = {}\n",
    "\n",
    "    for character in characters:\n",
    "        angles[character] = cosine_similarity(vector.reshape(1,-1), embeddings[character].reshape(1,-1))[0][0]\n",
    "\n",
    "    \n",
    "    \n",
    "    #Smaller angles between vectors produce larger cosine values, indicating greater cosine similarity\n",
    "\n",
    "    character = [i for i in angles if angles[i]==max(angles.values())]\n",
    "\n",
    "\n",
    "    return character[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.862502012558364\n",
      "accuracy:  18.33843181452262\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "real = []\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for idx, utterance in enumerate(test_set):\n",
    "    if len(utterance['speakers']) == 1:\n",
    "\n",
    "        total += 1\n",
    "\n",
    "        '''\n",
    "        if idx == 0:\n",
    "            line_embed = compute_embedding(\"\", utterance['transcript'])\n",
    "        else:\n",
    "            line_embed = compute_embedding(test_set1[idx-1]['transcript'], utterance['transcript'])'''\n",
    "        \n",
    "        '''if idx == 0:\n",
    "            #current, previous, next\n",
    "            line_embed = compute_embedding2(utterance['transcript'], \"\" , test_set1[idx+1]['transcript'])\n",
    "        elif idx == len(test_set)-1:\n",
    "            line_embed = compute_embedding2(utterance['transcript'], test_set1[idx-1]['transcript'] , \"\")\n",
    "        else:\n",
    "            line_embed = compute_embedding2(utterance['transcript'], test_set1[idx-1]['transcript'] , test_set1[idx+1]['transcript'])'''\n",
    "\n",
    "        '''\n",
    "        if idx == 0:\n",
    "            #current, previous, next\n",
    "            line_embed = compute_embedding2(utterance['transcript'], \"\" , \"\", test_set1[idx+1]['transcript'])\n",
    "        elif idx == 1:\n",
    "            line_embed = compute_embedding2(utterance['transcript'], \"\" , test_set1[idx-1]['transcript'],test_set1[idx+1]['transcript'])\n",
    "        elif idx == len(test_set)-1:\n",
    "            line_embed = compute_embedding2(utterance['transcript'],test_set1[idx-2]['transcript'] , test_set1[idx-1]['transcript'] , \"\")\n",
    "        else:\n",
    "            line_embed = compute_embedding2(utterance['transcript'], test_set1[idx-2]['transcript'] , test_set1[idx-1]['transcript'] , test_set1[idx+1]['transcript'])'''\n",
    "        \n",
    "        #current, previous2, previous, next, next2\n",
    "        if idx == 0:\n",
    "            line_embed = compute_embedding2(utterance['transcript'], \"\" , \"\", test_set1[idx+1]['transcript'], test_set1[idx+2]['transcript'])\n",
    "        elif idx == 1:\n",
    "            line_embed = compute_embedding2(utterance['transcript'], \"\" , test_set1[idx-1]['transcript'],test_set1[idx+1]['transcript'], test_set1[idx+2]['transcript'])\n",
    "        elif idx == len(test_set)-1:\n",
    "            line_embed = compute_embedding2(utterance['transcript'],test_set1[idx-2]['transcript'] , test_set1[idx-1]['transcript'] , \"\", \"\")\n",
    "        elif idx == len(test_set)-2:\n",
    "            line_embed = compute_embedding2(utterance['transcript'],test_set1[idx-2]['transcript'] , test_set1[idx-1]['transcript'] , test_set1[idx+1]['transcript'], \"\")\n",
    "        else:\n",
    "            line_embed = compute_embedding2(utterance['transcript'], test_set1[idx-2]['transcript'] , test_set1[idx-1]['transcript'] , test_set1[idx+1]['transcript'], test_set1[idx+2]['transcript'])\n",
    "    \n",
    "        pred = compute_similarity(line_embed)\n",
    "\n",
    "        predicted.append(pred)\n",
    "        real.append(utterance['speakers'][0])\n",
    "\n",
    "        if pred == utterance['speakers'][0]:\n",
    "            correct+=1\n",
    "        elif pred == \"others\" and utterance['speakers'][0] not in ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green']:\n",
    "            correct+=1\n",
    "\n",
    "                    \n",
    "print(accuracy_score(real, predicted)*100)\n",
    "\n",
    "accuracy = (float(correct)/total)*100\n",
    "\n",
    "print(\"accuracy: \", accuracy)\n",
    "\n",
    "#print(\"f1 score: \", f1_score(real, predicted, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nkproj",
   "language": "python",
   "name": "nkproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
