{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#characters = ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green', 'others']\n",
    "\n",
    "characters = ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green']\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for character in characters:\n",
    "    f = open(\"embeddings1/\" + character +\"6000\"+ \".txt\", \"r\")\n",
    "    lines = f.readlines()\n",
    "    sentences.append(lines)\n",
    "\n",
    "i=0\n",
    "embeddings = {}\n",
    "for character in characters:\n",
    "    embeddings[character] = model.encode(sentences[i])\n",
    "    i+=1\n",
    "\n",
    "\n",
    "#to confirm number of lines used to create the embedding\n",
    "print(len(embeddings['Monica Geller']))\n",
    "\n",
    "for character in characters:\n",
    "\n",
    "    mean = embeddings[character][0]\n",
    "    for embedding in embeddings[character]:\n",
    "        mean += embedding\n",
    "    \n",
    "    embeddings[character] = mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set 1 = season 10\n",
    "\n",
    "with open('sets/test_set1.json') as f:\n",
    "    test_set1 = json.load(f)\n",
    "\n",
    "################################################################\n",
    "\n",
    "#test set 2 = 10% of each season\n",
    "\n",
    "with open('sets/test_set2.json') as f:\n",
    "    test_set2 = json.load(f)\n",
    "\n",
    "with open('sets/train_set2.json') as f:\n",
    "    train_set2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_calculator(lines):    \n",
    "    # Compute the term frequency of each word in each sentence\n",
    "    tf = []\n",
    "    \n",
    "    for sentence in lines:\n",
    "        tf_sentence = {}\n",
    "        for word in sentence.split():\n",
    "            tf_sentence[word] = tf_sentence.get(word, 0) + 1\n",
    "        tf.append(tf_sentence)\n",
    "\n",
    "    # Compute the inverse document frequency of each word\n",
    "    idf = {}\n",
    "    for sentence in lines:\n",
    "        for word in sentence.split():\n",
    "            idf[word] = idf.get(word, 0) + 1\n",
    "    for word in idf:\n",
    "        idf[word] = math.log(len(lines) / idf[word])\n",
    "\n",
    "    # Compute the tf-idf of each sentence\n",
    "    tf_idf = []\n",
    "    for i, sentence in enumerate(lines):\n",
    "        tf_idf_sentence = 0\n",
    "        for word in tf[i]:\n",
    "            tf_idf_sentence += tf[i][word] * idf[word]\n",
    "        tf_idf.append(tf_idf_sentence)\n",
    "    \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = []\n",
    "for episode in test_set1['episodes']:\n",
    "    for scene in episode['scenes']:\n",
    "        for utterance in scene['utterances']:\n",
    "            if utterance['speakers'] != []:\n",
    "                all_lines.append(utterance['transcript'])\n",
    "\n",
    "#remove duplicates\n",
    "all_lines = list( dict.fromkeys(all_lines) )\n",
    "\n",
    "tf_idf = tf_idf_calculator(all_lines)\n",
    "\n",
    "all_lines_tf_idf = []\n",
    "\n",
    "#format: all_lines_tf_idf = [...,[tf-idf score ,sentence],...]\n",
    "for i,sentence in enumerate(all_lines):\n",
    "    cell = []\n",
    "    cell= [tf_idf[i], sentence]\n",
    "    all_lines_tf_idf.append(cell)\n",
    "    \n",
    "# lower -> higher     \n",
    "sorted_list = sorted(all_lines_tf_idf)\n",
    "\n",
    "tf_idfs_utterances = []\n",
    "\n",
    "#get only the strings\n",
    "for elem in sorted_list:\n",
    "    tf_idfs_utterances.append(elem[1])\n",
    "\n",
    "\n",
    "#for elem in sorted_list:\n",
    "#    print(elem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(vector):\n",
    "\n",
    "    angles = {}\n",
    "\n",
    "    for character in characters:\n",
    "        angles[character] = cosine_similarity(vector.reshape(1,-1), embeddings[character].reshape(1,-1))[0][0]\n",
    "\n",
    "    \n",
    "    \n",
    "    #Smaller angles between vectors produce larger cosine values, indicating greater cosine similarity\n",
    "\n",
    "    character = [i for i in angles if angles[i]==max(angles.values())]\n",
    "\n",
    "\n",
    "    return character[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(utterance[\u001b[39m'\u001b[39m\u001b[39mspeakers\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     12\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 14\u001b[0m     line_embed \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(utterance[\u001b[39m'\u001b[39;49m\u001b[39mtranscript\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     16\u001b[0m     pred \u001b[39m=\u001b[39m compute_similarity(line_embed)\n\u001b[1;32m     18\u001b[0m     predicted\u001b[39m.\u001b[39mappend(pred)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:153\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     device \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_device\n\u001b[0;32m--> 153\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort([\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_length(sen) \u001b[39mfor\u001b[39;00m sen \u001b[39min\u001b[39;00m sentences])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:819\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m--> 819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39;49mno_grad():\n\u001b[1;32m    820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:50\u001b[0m, in \u001b[0;36mno_grad.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_jit_internal\u001b[39m.\u001b[39mis_scripting():\n\u001b[0;32m---> 50\u001b[0m         \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "real = []\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for episode in test_set['episodes']:\n",
    "    for scene in episode['scenes']:\n",
    "        for utterance in scene['utterances']:\n",
    "            if len(utterance['speakers']) == 1:\n",
    "\n",
    "                total += 1\n",
    "\n",
    "                line_embed = model.encode(utterance['transcript'])\n",
    "    \n",
    "                pred = compute_similarity(line_embed)\n",
    "\n",
    "                predicted.append(pred)\n",
    "                real.append(utterance['speakers'][0])\n",
    "\n",
    "                if pred == utterance['speakers'][0]:\n",
    "                    correct+=1\n",
    "                elif pred == \"others\" and utterance['speakers'][0] not in ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green']:\n",
    "                    correct+=1\n",
    "\n",
    "                #print(\"predicted :\", pred)\n",
    "                #print(\"real:\", utterance['speakers'][0])\n",
    "\n",
    "                    \n",
    "print(accuracy_score(real, predicted)*100)\n",
    "\n",
    "accuracy = (float(correct)/total)*100\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "print(len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.88788980294624\n",
      "17.88788980294624\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "real = []\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for utterance in test_set1:\n",
    "    if len(utterance['speakers']) == 1:\n",
    "\n",
    "        total += 1\n",
    "\n",
    "        line_embed = model.encode(utterance['transcript'])\n",
    "    \n",
    "        pred = compute_similarity(line_embed)\n",
    "\n",
    "        predicted.append(pred)\n",
    "        real.append(utterance['speakers'][0])\n",
    "\n",
    "        if pred == utterance['speakers'][0]:\n",
    "            correct+=1\n",
    "        elif pred == \"others\" and utterance['speakers'][0] not in ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green']:\n",
    "            correct+=1\n",
    "\n",
    "                    \n",
    "print(accuracy_score(real, predicted)*100)\n",
    "\n",
    "accuracy = (float(correct)/total)*100\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "print(len(characters))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier WITH filter in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.54855994641661\n",
      "17.54855994641661\n",
      "6\n",
      "10\n",
      "14.310311842356995\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "real = []\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "threshold = 10\n",
    "\n",
    "all = 0\n",
    "for episode in test_set['episodes']:\n",
    "    for scene in episode['scenes']:\n",
    "        for utterance in scene['utterances']:\n",
    "            if len(utterance['speakers']) == 1:\n",
    "                all += 1\n",
    "\n",
    "                index = tf_idfs_utterances.index(utterance['transcript'])\n",
    "\n",
    "                if sorted_list[index][0] >= threshold:\n",
    "\n",
    "                    total += 1\n",
    "\n",
    "                    line_embed = model.encode(utterance['transcript'])\n",
    "        \n",
    "                    pred = compute_similarity(line_embed)\n",
    "\n",
    "                    predicted.append(pred)\n",
    "                    real.append(utterance['speakers'][0])\n",
    "\n",
    "                    if pred == utterance['speakers'][0]:\n",
    "                        correct+=1\n",
    "                    elif pred == \"others\" and utterance['speakers'][0] not in ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green']:\n",
    "                        correct+=1\n",
    "\n",
    "\n",
    "                    \n",
    "print(accuracy_score(real, predicted)*100)\n",
    "\n",
    "accuracy = (float(correct)/total)*100\n",
    "\n",
    "print(accuracy)\n",
    "print(len(characters))\n",
    "print(threshold)\n",
    "\n",
    "print(100 - float(total/all)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nkproj",
   "language": "python",
   "name": "nkproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
