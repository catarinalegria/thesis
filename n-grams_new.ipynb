{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/catarina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import string\n",
    "import operator\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(sentence: str, n: int):\n",
    "    # Convert sentence to lowercase and remove punctuation\n",
    "    import string\n",
    "    sentence = sentence.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Split sentence into words\n",
    "    words = sentence.split()\n",
    "        \n",
    "    # Generate N-grams\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngrams.append(' '.join(words[i:i+n]))\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(seasons, characters,n):\n",
    "    values = []\n",
    "    \n",
    "    for character in characters:\n",
    "            lines = []\n",
    "            counts = []\n",
    "            words = []\n",
    "\n",
    "            for season in seasons:\n",
    "                for episode in season['episodes']:\n",
    "                    for scene in episode['scenes']:\n",
    "                        for utterance in scene['utterances']:\n",
    "                            if utterance['speakers'] != []:\n",
    "                                if character in utterance['speakers']:\n",
    "                                    lines.append(utterance['transcript'])\n",
    "\n",
    "            n_grams = []\n",
    "\n",
    "            for line in lines:\n",
    "                #remve punctuation\n",
    "                line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "                if n==1:\n",
    "                    line = word_tokenize(line)\n",
    "\n",
    "                    for word in line:\n",
    "                        if word not in stop_words and word != \"â€™\":\n",
    "                            words.append(word.lower())    \n",
    "                \n",
    "                else:\n",
    "                    n_gram = generate_ngrams(line,n)\n",
    "                    for word in n_gram:\n",
    "                        n_grams.append(word)\n",
    "\n",
    "\n",
    "            if n==1:\n",
    "                counts = Counter(words)\n",
    "                counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            else:\n",
    "                counter = {}\n",
    "                for elem in n_grams:\n",
    "                    if elem in counter:\n",
    "                        counter[elem]+=1\n",
    "                    else:\n",
    "                        counter[elem]=1\n",
    "                counter_sorted = sorted(counter.items(), key=operator.itemgetter(1))\n",
    "                counts = counter_sorted[::-1]\n",
    "\n",
    "\n",
    "            values.append(counts)\n",
    "\n",
    "    dict = {key: value for key, value in zip(characters, values)}\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = []\n",
    "\n",
    "for i in range(1,11):\n",
    "\n",
    "    # load season i\n",
    "    if i<10:\n",
    "        json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_0'+str(i)+'.json'\n",
    "    else:\n",
    "        json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_10.json'\n",
    "    \n",
    "    r = requests.get(json_file)\n",
    "    seasons.append(json.loads(r.text))\n",
    "\n",
    "all_lines = []\n",
    "for season in seasons:\n",
    "    for episode in season['episodes']:\n",
    "        for scene in episode['scenes']:\n",
    "            for utterance in scene['utterances']:\n",
    "                if utterance['speakers'] != []:\n",
    "                    all_lines.append(utterance['transcript'])\n",
    "\n",
    "#we can do this manually\n",
    "characters = ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "monica_lines = []\n",
    "joey_lines = []\n",
    "chandler_lines = []\n",
    "phoebe_lines = []\n",
    "ross_lines = []\n",
    "rachel_lines = []\n",
    "\n",
    "for season in seasons:\n",
    "    for episode in season['episodes']:\n",
    "        for scene in episode['scenes']:\n",
    "            for utterance in scene['utterances']:\n",
    "                if utterance['speakers'] != []:\n",
    "                    if \"Monica Geller\" in utterance['speakers']:\n",
    "                        monica_lines.append(utterance['transcript'])\n",
    "                    if \"Joey Tribbiani\" in utterance['speakers']:\n",
    "                        joey_lines.append(utterance['transcript'])\n",
    "                    if \"Chandler Bing\" in utterance['speakers']:\n",
    "                        chandler_lines.append(utterance['transcript'])\n",
    "                    if \"Phoebe Buffay\" in utterance['speakers']:\n",
    "                        phoebe_lines.append(utterance['transcript'])\n",
    "                    if \"Ross Geller\" in utterance['speakers']:\n",
    "                        ross_lines.append(utterance['transcript'])\n",
    "                    if \"Rachel Green\" in utterance['speakers']:\n",
    "                        rachel_lines.append(utterance['transcript'])\n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_bdfca\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bdfca_level0_col0\" class=\"col_heading level0 col0\" >Monica Geller</th>\n",
       "      <th id=\"T_bdfca_level0_col1\" class=\"col_heading level0 col1\" >Joey Tribbiani</th>\n",
       "      <th id=\"T_bdfca_level0_col2\" class=\"col_heading level0 col2\" >Chandler Bing</th>\n",
       "      <th id=\"T_bdfca_level0_col3\" class=\"col_heading level0 col3\" >Phoebe Buffay</th>\n",
       "      <th id=\"T_bdfca_level0_col4\" class=\"col_heading level0 col4\" >Ross Geller</th>\n",
       "      <th id=\"T_bdfca_level0_col5\" class=\"col_heading level0 col5\" >Rachel Green</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bdfca_row0_col0\" class=\"data row0 col0\" >('what are you doing', 46)</td>\n",
       "      <td id=\"T_bdfca_row0_col1\" class=\"data row0 col1\" >('what are you doing', 25)</td>\n",
       "      <td id=\"T_bdfca_row0_col2\" class=\"data row0 col2\" >('no no no no', 41)</td>\n",
       "      <td id=\"T_bdfca_row0_col3\" class=\"data row0 col3\" >('i know i know', 20)</td>\n",
       "      <td id=\"T_bdfca_row0_col4\" class=\"data row0 col4\" >('what are you doing', 38)</td>\n",
       "      <td id=\"T_bdfca_row0_col5\" class=\"data row0 col5\" >('i dont know i', 33)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_bdfca_row1_col0\" class=\"data row1 col0\" >('no no no no', 28)</td>\n",
       "      <td id=\"T_bdfca_row1_col1\" class=\"data row1 col1\" >('days of our lives', 25)</td>\n",
       "      <td id=\"T_bdfca_row1_col2\" class=\"data row1 col2\" >('what are you doing', 27)</td>\n",
       "      <td id=\"T_bdfca_row1_col3\" class=\"data row1 col3\" >('i dont know i', 18)</td>\n",
       "      <td id=\"T_bdfca_row1_col4\" class=\"data row1 col4\" >('i dont know what', 21)</td>\n",
       "      <td id=\"T_bdfca_row1_col5\" class=\"data row1 col5\" >('i know i know', 33)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_bdfca_row2_col0\" class=\"data row2 col0\" >('are you talking about', 22)</td>\n",
       "      <td id=\"T_bdfca_row2_col1\" class=\"data row2 col1\" >('no no no no', 24)</td>\n",
       "      <td id=\"T_bdfca_row2_col2\" class=\"data row2 col2\" >('you dont have to', 18)</td>\n",
       "      <td id=\"T_bdfca_row2_col3\" class=\"data row2 col3\" >('what are you doing', 16)</td>\n",
       "      <td id=\"T_bdfca_row2_col4\" class=\"data row2 col4\" >('i dont want to', 19)</td>\n",
       "      <td id=\"T_bdfca_row2_col5\" class=\"data row2 col5\" >('what are you doing', 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_bdfca_row3_col0\" class=\"data row3 col0\" >('what are you talking', 18)</td>\n",
       "      <td id=\"T_bdfca_row3_col1\" class=\"data row3 col1\" >('i dont want to', 24)</td>\n",
       "      <td id=\"T_bdfca_row3_col2\" class=\"data row3 col2\" >('i dont know what', 17)</td>\n",
       "      <td id=\"T_bdfca_row3_col3\" class=\"data row3 col3\" >('la la la la', 16)</td>\n",
       "      <td id=\"T_bdfca_row3_col4\" class=\"data row3 col4\" >('i dont i dont', 16)</td>\n",
       "      <td id=\"T_bdfca_row3_col5\" class=\"data row3 col5\" >('oh my god oh', 31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_bdfca_row4_col0\" class=\"data row4 col0\" >('god oh my god', 17)</td>\n",
       "      <td id=\"T_bdfca_row4_col1\" class=\"data row4 col1\" >('all right all right', 20)</td>\n",
       "      <td id=\"T_bdfca_row4_col2\" class=\"data row4 col2\" >('i dont want to', 13)</td>\n",
       "      <td id=\"T_bdfca_row4_col3\" class=\"data row4 col3\" >('i dont want to', 16)</td>\n",
       "      <td id=\"T_bdfca_row4_col4\" class=\"data row4 col4\" >('i dont know i', 15)</td>\n",
       "      <td id=\"T_bdfca_row4_col5\" class=\"data row4 col5\" >('my god oh my', 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_bdfca_row5_col0\" class=\"data row5 col0\" >('my god oh my', 17)</td>\n",
       "      <td id=\"T_bdfca_row5_col1\" class=\"data row5 col1\" >('you know what i', 15)</td>\n",
       "      <td id=\"T_bdfca_row5_col2\" class=\"data row5 col2\" >('i want you to', 12)</td>\n",
       "      <td id=\"T_bdfca_row5_col3\" class=\"data row5 col3\" >('oh my god i', 15)</td>\n",
       "      <td id=\"T_bdfca_row5_col4\" class=\"data row5 col4\" >('i know i know', 15)</td>\n",
       "      <td id=\"T_bdfca_row5_col5\" class=\"data row5 col5\" >('god oh my god', 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_bdfca_row6_col0\" class=\"data row6 col0\" >('oh my god oh', 17)</td>\n",
       "      <td id=\"T_bdfca_row6_col1\" class=\"data row6 col1\" >('i know i know', 14)</td>\n",
       "      <td id=\"T_bdfca_row6_col2\" class=\"data row6 col2\" >('you want me to', 11)</td>\n",
       "      <td id=\"T_bdfca_row6_col3\" class=\"data row6 col3\" >('you dont have to', 15)</td>\n",
       "      <td id=\"T_bdfca_row6_col4\" class=\"data row6 col4\" >('i cant believe this', 15)</td>\n",
       "      <td id=\"T_bdfca_row6_col5\" class=\"data row6 col5\" >('you know what i', 22)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_bdfca_row7_col0\" class=\"data row7 col0\" >('what do you think', 17)</td>\n",
       "      <td id=\"T_bdfca_row7_col1\" class=\"data row7 col1\" >('i want you to', 13)</td>\n",
       "      <td id=\"T_bdfca_row7_col2\" class=\"data row7 col2\" >('oh no no no', 9)</td>\n",
       "      <td id=\"T_bdfca_row7_col3\" class=\"data row7 col3\" >('do you want to', 13)</td>\n",
       "      <td id=\"T_bdfca_row7_col4\" class=\"data row7 col4\" >('you know what i', 15)</td>\n",
       "      <td id=\"T_bdfca_row7_col5\" class=\"data row7 col5\" >('no no no no', 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_bdfca_row8_col0\" class=\"data row8 col0\" >('you dont have to', 17)</td>\n",
       "      <td id=\"T_bdfca_row8_col1\" class=\"data row8 col1\" >('to talk to you', 13)</td>\n",
       "      <td id=\"T_bdfca_row8_col2\" class=\"data row8 col2\" >('ow ow ow ow', 9)</td>\n",
       "      <td id=\"T_bdfca_row8_col3\" class=\"data row8 col3\" >('no no no no', 13)</td>\n",
       "      <td id=\"T_bdfca_row8_col4\" class=\"data row8 col4\" >('what do you think', 14)</td>\n",
       "      <td id=\"T_bdfca_row8_col5\" class=\"data row8 col5\" >('oh my god i', 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_bdfca_row9_col0\" class=\"data row9 col0\" >('why dont you just', 15)</td>\n",
       "      <td id=\"T_bdfca_row9_col1\" class=\"data row9 col1\" >('i dont know i', 13)</td>\n",
       "      <td id=\"T_bdfca_row9_col2\" class=\"data row9 col2\" >('i think we should', 8)</td>\n",
       "      <td id=\"T_bdfca_row9_col3\" class=\"data row9 col3\" >('you know what i', 13)</td>\n",
       "      <td id=\"T_bdfca_row9_col4\" class=\"data row9 col4\" >('you know what you', 14)</td>\n",
       "      <td id=\"T_bdfca_row9_col5\" class=\"data row9 col5\" >('i dont want to', 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_bdfca_row10_col0\" class=\"data row10 col0\" >('do you want to', 14)</td>\n",
       "      <td id=\"T_bdfca_row10_col1\" class=\"data row10 col1\" >('i dont know what', 12)</td>\n",
       "      <td id=\"T_bdfca_row10_col2\" class=\"data row10 col2\" >('want you to know', 8)</td>\n",
       "      <td id=\"T_bdfca_row10_col3\" class=\"data row10 col3\" >('im sorry im sorry', 11)</td>\n",
       "      <td id=\"T_bdfca_row10_col4\" class=\"data row10 col4\" >('come on come on', 13)</td>\n",
       "      <td id=\"T_bdfca_row10_col5\" class=\"data row10 col5\" >('ow ow ow ow', 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_bdfca_row11_col0\" class=\"data row11 col0\" >('it ill get it', 13)</td>\n",
       "      <td id=\"T_bdfca_row11_col1\" class=\"data row11 col1\" >('i think im gonna', 11)</td>\n",
       "      <td id=\"T_bdfca_row11_col2\" class=\"data row11 col2\" >('no no no i', 8)</td>\n",
       "      <td id=\"T_bdfca_row11_col3\" class=\"data row11 col3\" >('i cant believe you', 11)</td>\n",
       "      <td id=\"T_bdfca_row11_col4\" class=\"data row11 col4\" >('no no no no', 13)</td>\n",
       "      <td id=\"T_bdfca_row11_col5\" class=\"data row11 col5\" >('thank you thank you', 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_bdfca_row12_col0\" class=\"data row12 col0\" >('get it ill get', 13)</td>\n",
       "      <td id=\"T_bdfca_row12_col1\" class=\"data row12 col1\" >('what do you say', 11)</td>\n",
       "      <td id=\"T_bdfca_row12_col2\" class=\"data row12 col2\" >('no no i just', 8)</td>\n",
       "      <td id=\"T_bdfca_row12_col3\" class=\"data row12 col3\" >('oh my god oh', 10)</td>\n",
       "      <td id=\"T_bdfca_row12_col4\" class=\"data row12 col4\" >('to talk to you', 12)</td>\n",
       "      <td id=\"T_bdfca_row12_col5\" class=\"data row12 col5\" >('why dont you just', 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_bdfca_row13_col0\" class=\"data row13 col0\" >('ill get it ill', 13)</td>\n",
       "      <td id=\"T_bdfca_row13_col1\" class=\"data row13 col1\" >('are you talking about', 11)</td>\n",
       "      <td id=\"T_bdfca_row13_col2\" class=\"data row13 col2\" >('what do you say', 8)</td>\n",
       "      <td id=\"T_bdfca_row13_col3\" class=\"data row13 col3\" >('i dont know what', 10)</td>\n",
       "      <td id=\"T_bdfca_row13_col4\" class=\"data row13 col4\" >('are you doing here', 12)</td>\n",
       "      <td id=\"T_bdfca_row13_col5\" class=\"data row13 col5\" >('are you talking about', 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdfca_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_bdfca_row14_col0\" class=\"data row14 col0\" >('i want you to', 13)</td>\n",
       "      <td id=\"T_bdfca_row14_col1\" class=\"data row14 col1\" >('you want me to', 10)</td>\n",
       "      <td id=\"T_bdfca_row14_col2\" class=\"data row14 col2\" >('are you doing here', 8)</td>\n",
       "      <td id=\"T_bdfca_row14_col3\" class=\"data row14 col3\" >('i have to go', 10)</td>\n",
       "      <td id=\"T_bdfca_row14_col4\" class=\"data row14 col4\" >('i want you to', 12)</td>\n",
       "      <td id=\"T_bdfca_row14_col5\" class=\"data row14 col5\" >('i think im gonna', 14)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7d6d6d42b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change 3rd argument to 1 for unigrams, 2 for bigrams, 3 for trigrams etc\n",
    "dict = build_dict(seasons, characters, 4)\n",
    "\n",
    "df = pd.DataFrame.from_dict(dict, orient='index')\n",
    "\n",
    "df_new = df.iloc[:, 0:15]\n",
    "\n",
    "# displaying the DataFrame\n",
    "df_new.T.style"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-grams generics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict_generic(seasons,n):      \n",
    "    counts = []\n",
    "    words = []\n",
    "\n",
    "\n",
    "    n_grams = []\n",
    "\n",
    "    for line in all_lines:\n",
    "        #remve punctuation\n",
    "        line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        if n==1:\n",
    "            line = word_tokenize(line)\n",
    "\n",
    "            for word in line:\n",
    "                if word not in stop_words and word != \"â€™\":\n",
    "                    words.append(word.lower())    \n",
    "                \n",
    "        else:\n",
    "            n_gram = generate_ngrams(line,n)\n",
    "            for word in n_gram:\n",
    "                n_grams.append(word)\n",
    "\n",
    "\n",
    "    if n==1:\n",
    "        counts = Counter(words)\n",
    "        counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "    else:\n",
    "        counter = {}\n",
    "        for elem in n_grams:\n",
    "            if elem in counter:\n",
    "                counter[elem]+=1\n",
    "            else:\n",
    "                counter[elem]=1\n",
    "        counter_sorted = sorted(counter.items(), key=operator.itemgetter(1))\n",
    "        counts = counter_sorted[::-1]\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_16efe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_16efe_level0_col0\" class=\"col_heading level0 col0\" >word</th>\n",
       "      <th id=\"T_16efe_level0_col1\" class=\"col_heading level0 col1\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_16efe_row0_col0\" class=\"data row0 col0\" >no no no no no</td>\n",
       "      <td id=\"T_16efe_row0_col1\" class=\"data row0 col1\" >66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_16efe_row1_col0\" class=\"data row1 col0\" >what are you doing here</td>\n",
       "      <td id=\"T_16efe_row1_col1\" class=\"data row1 col1\" >62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_16efe_row2_col0\" class=\"data row2 col0\" >what are you talking about</td>\n",
       "      <td id=\"T_16efe_row2_col1\" class=\"data row2 col1\" >57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_16efe_row3_col0\" class=\"data row3 col0\" >my god oh my god</td>\n",
       "      <td id=\"T_16efe_row3_col1\" class=\"data row3 col1\" >54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_16efe_row4_col0\" class=\"data row4 col0\" >oh my god oh my</td>\n",
       "      <td id=\"T_16efe_row4_col1\" class=\"data row4 col1\" >52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_16efe_row5_col0\" class=\"data row5 col0\" >do you want me to</td>\n",
       "      <td id=\"T_16efe_row5_col1\" class=\"data row5 col1\" >38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_16efe_row6_col0\" class=\"data row6 col0\" >i dont know what to</td>\n",
       "      <td id=\"T_16efe_row6_col1\" class=\"data row6 col1\" >34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_16efe_row7_col0\" class=\"data row7 col0\" >can i talk to you</td>\n",
       "      <td id=\"T_16efe_row7_col1\" class=\"data row7 col1\" >33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_16efe_row8_col0\" class=\"data row8 col0\" >to talk to you about</td>\n",
       "      <td id=\"T_16efe_row8_col1\" class=\"data row8 col1\" >32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_16efe_row9_col0\" class=\"data row9 col0\" >what are you gonna do</td>\n",
       "      <td id=\"T_16efe_row9_col1\" class=\"data row9 col1\" >31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_16efe_row10_col0\" class=\"data row10 col0\" >need to talk to you</td>\n",
       "      <td id=\"T_16efe_row10_col1\" class=\"data row10 col1\" >27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_16efe_row11_col0\" class=\"data row11 col0\" >i dont know i mean</td>\n",
       "      <td id=\"T_16efe_row11_col1\" class=\"data row11 col1\" >27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_16efe_row12_col0\" class=\"data row12 col0\" >thank you so much for</td>\n",
       "      <td id=\"T_16efe_row12_col1\" class=\"data row12 col1\" >26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_16efe_row13_col0\" class=\"data row13 col0\" >what am i gonna do</td>\n",
       "      <td id=\"T_16efe_row13_col1\" class=\"data row13 col1\" >26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_16efe_row14_col0\" class=\"data row14 col0\" >ow ow ow ow ow</td>\n",
       "      <td id=\"T_16efe_row14_col1\" class=\"data row14 col1\" >25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_16efe_row15_col0\" class=\"data row15 col0\" >talk to you for a</td>\n",
       "      <td id=\"T_16efe_row15_col1\" class=\"data row15 col1\" >25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_16efe_row16_col0\" class=\"data row16 col0\" >what are you guys doing</td>\n",
       "      <td id=\"T_16efe_row16_col1\" class=\"data row16 col1\" >25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_16efe_row17_col0\" class=\"data row17 col0\" >i dont know i dont</td>\n",
       "      <td id=\"T_16efe_row17_col1\" class=\"data row17 col1\" >24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_16efe_row18_col0\" class=\"data row18 col0\" >what are we gonna do</td>\n",
       "      <td id=\"T_16efe_row18_col1\" class=\"data row18 col1\" >22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16efe_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_16efe_row19_col0\" class=\"data row19 col0\" >oh my god this is</td>\n",
       "      <td id=\"T_16efe_row19_col1\" class=\"data row19 col1\" >21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7d882c1ae0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change 3rd argument to 1 for unigrams, 2 for bigrams, 3 for trigrams etc\n",
    "counts = build_dict_generic(seasons, 5)\n",
    "\n",
    "df = pd.DataFrame(counts, columns = ['word', 'count'])\n",
    "\n",
    "df_new = df.iloc[:20]\n",
    "\n",
    "# displaying the DataFrame\n",
    "df_new.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_191de\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Character</th>\n",
       "      <th id=\"T_191de_level0_col0\" class=\"col_heading level0 col0\" >Monica Geller</th>\n",
       "      <th id=\"T_191de_level0_col1\" class=\"col_heading level0 col1\" >Joey Tribbiani</th>\n",
       "      <th id=\"T_191de_level0_col2\" class=\"col_heading level0 col2\" >Chandler Bing</th>\n",
       "      <th id=\"T_191de_level0_col3\" class=\"col_heading level0 col3\" >Phoebe Buffay</th>\n",
       "      <th id=\"T_191de_level0_col4\" class=\"col_heading level0 col4\" >Ross Geller</th>\n",
       "      <th id=\"T_191de_level0_col5\" class=\"col_heading level0 col5\" >Rachel Green</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level1\" >N-gram</th>\n",
       "      <th id=\"T_191de_level1_col0\" class=\"col_heading level1 col0\" >mean_tfidf_score</th>\n",
       "      <th id=\"T_191de_level1_col1\" class=\"col_heading level1 col1\" >mean_tfidf_score</th>\n",
       "      <th id=\"T_191de_level1_col2\" class=\"col_heading level1 col2\" >mean_tfidf_score</th>\n",
       "      <th id=\"T_191de_level1_col3\" class=\"col_heading level1 col3\" >mean_tfidf_score</th>\n",
       "      <th id=\"T_191de_level1_col4\" class=\"col_heading level1 col4\" >mean_tfidf_score</th>\n",
       "      <th id=\"T_191de_level1_col5\" class=\"col_heading level1 col5\" >mean_tfidf_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_191de_level0_row0\" class=\"row_heading level0 row0\" >you</th>\n",
       "      <td id=\"T_191de_row0_col0\" class=\"data row0 col0\" >0.056524</td>\n",
       "      <td id=\"T_191de_row0_col1\" class=\"data row0 col1\" >0.050915</td>\n",
       "      <td id=\"T_191de_row0_col2\" class=\"data row0 col2\" >0.050523</td>\n",
       "      <td id=\"T_191de_row0_col3\" class=\"data row0 col3\" >0.054497</td>\n",
       "      <td id=\"T_191de_row0_col4\" class=\"data row0 col4\" >0.051138</td>\n",
       "      <td id=\"T_191de_row0_col5\" class=\"data row0 col5\" >0.052314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_191de_level0_row1\" class=\"row_heading level0 row1\" >what</th>\n",
       "      <td id=\"T_191de_row1_col0\" class=\"data row1 col0\" >0.038414</td>\n",
       "      <td id=\"T_191de_row1_col1\" class=\"data row1 col1\" >0.034056</td>\n",
       "      <td id=\"T_191de_row1_col2\" class=\"data row1 col2\" >0.033335</td>\n",
       "      <td id=\"T_191de_row1_col3\" class=\"data row1 col3\" >0.031392</td>\n",
       "      <td id=\"T_191de_row1_col4\" class=\"data row1 col4\" >0.040802</td>\n",
       "      <td id=\"T_191de_row1_col5\" class=\"data row1 col5\" >0.044246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_191de_level0_row2\" class=\"row_heading level0 row2\" >it</th>\n",
       "      <td id=\"T_191de_row2_col0\" class=\"data row2 col0\" >0.035085</td>\n",
       "      <td id=\"T_191de_row2_col1\" class=\"data row2 col1\" >0.034172</td>\n",
       "      <td id=\"T_191de_row2_col2\" class=\"data row2 col2\" >0.033797</td>\n",
       "      <td id=\"T_191de_row2_col3\" class=\"data row2 col3\" >0.034604</td>\n",
       "      <td id=\"T_191de_row2_col4\" class=\"data row2 col4\" >0.033607</td>\n",
       "      <td id=\"T_191de_row2_col5\" class=\"data row2 col5\" >0.031570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_191de_level0_row3\" class=\"row_heading level0 row3\" >the</th>\n",
       "      <td id=\"T_191de_row3_col0\" class=\"data row3 col0\" >0.033289</td>\n",
       "      <td id=\"T_191de_row3_col1\" class=\"data row3 col1\" >0.034224</td>\n",
       "      <td id=\"T_191de_row3_col2\" class=\"data row3 col2\" >0.036335</td>\n",
       "      <td id=\"T_191de_row3_col3\" class=\"data row3 col3\" >0.031446</td>\n",
       "      <td id=\"T_191de_row3_col4\" class=\"data row3 col4\" >0.032693</td>\n",
       "      <td id=\"T_191de_row3_col5\" class=\"data row3 col5\" >0.027673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_191de_level0_row4\" class=\"row_heading level0 row4\" >that</th>\n",
       "      <td id=\"T_191de_row4_col0\" class=\"data row4 col0\" >0.029591</td>\n",
       "      <td id=\"T_191de_row4_col1\" class=\"data row4 col1\" >0.029443</td>\n",
       "      <td id=\"T_191de_row4_col2\" class=\"data row4 col2\" >0.032132</td>\n",
       "      <td id=\"T_191de_row4_col3\" class=\"data row4 col3\" >0.029122</td>\n",
       "      <td id=\"T_191de_row4_col4\" class=\"data row4 col4\" >0.028249</td>\n",
       "      <td id=\"T_191de_row4_col5\" class=\"data row4 col5\" >0.030775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_191de_level0_row5\" class=\"row_heading level0 row5\" >oh</th>\n",
       "      <td id=\"T_191de_row5_col0\" class=\"data row5 col0\" >0.029343</td>\n",
       "      <td id=\"T_191de_row5_col1\" class=\"data row5 col1\" >0.025423</td>\n",
       "      <td id=\"T_191de_row5_col2\" class=\"data row5 col2\" >nan</td>\n",
       "      <td id=\"T_191de_row5_col3\" class=\"data row5 col3\" >0.043714</td>\n",
       "      <td id=\"T_191de_row5_col4\" class=\"data row5 col4\" >0.026198</td>\n",
       "      <td id=\"T_191de_row5_col5\" class=\"data row5 col5\" >0.044936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_191de_level0_row6\" class=\"row_heading level0 row6\" >to</th>\n",
       "      <td id=\"T_191de_row6_col0\" class=\"data row6 col0\" >0.026531</td>\n",
       "      <td id=\"T_191de_row6_col1\" class=\"data row6 col1\" >0.026339</td>\n",
       "      <td id=\"T_191de_row6_col2\" class=\"data row6 col2\" >0.027659</td>\n",
       "      <td id=\"T_191de_row6_col3\" class=\"data row6 col3\" >nan</td>\n",
       "      <td id=\"T_191de_row6_col4\" class=\"data row6 col4\" >0.027042</td>\n",
       "      <td id=\"T_191de_row6_col5\" class=\"data row6 col5\" >0.025975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_191de_level0_row7\" class=\"row_heading level0 row7\" >hey</th>\n",
       "      <td id=\"T_191de_row7_col0\" class=\"data row7 col0\" >0.025297</td>\n",
       "      <td id=\"T_191de_row7_col1\" class=\"data row7 col1\" >0.048687</td>\n",
       "      <td id=\"T_191de_row7_col2\" class=\"data row7 col2\" >0.033730</td>\n",
       "      <td id=\"T_191de_row7_col3\" class=\"data row7 col3\" >0.033275</td>\n",
       "      <td id=\"T_191de_row7_col4\" class=\"data row7 col4\" >0.035439</td>\n",
       "      <td id=\"T_191de_row7_col5\" class=\"data row7 col5\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_191de_level0_row8\" class=\"row_heading level0 row8\" >okay</th>\n",
       "      <td id=\"T_191de_row8_col0\" class=\"data row8 col0\" >0.024671</td>\n",
       "      <td id=\"T_191de_row8_col1\" class=\"data row8 col1\" >nan</td>\n",
       "      <td id=\"T_191de_row8_col2\" class=\"data row8 col2\" >0.027627</td>\n",
       "      <td id=\"T_191de_row8_col3\" class=\"data row8 col3\" >0.028614</td>\n",
       "      <td id=\"T_191de_row8_col4\" class=\"data row8 col4\" >nan</td>\n",
       "      <td id=\"T_191de_row8_col5\" class=\"data row8 col5\" >0.027538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_191de_level0_row9\" class=\"row_heading level0 row9\" >no</th>\n",
       "      <td id=\"T_191de_row9_col0\" class=\"data row9 col0\" >0.022041</td>\n",
       "      <td id=\"T_191de_row9_col1\" class=\"data row9 col1\" >0.026416</td>\n",
       "      <td id=\"T_191de_row9_col2\" class=\"data row9 col2\" >0.025110</td>\n",
       "      <td id=\"T_191de_row9_col3\" class=\"data row9 col3\" >0.027078</td>\n",
       "      <td id=\"T_191de_row9_col4\" class=\"data row9 col4\" >0.031049</td>\n",
       "      <td id=\"T_191de_row9_col5\" class=\"data row9 col5\" >0.024626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_191de_level0_row10\" class=\"row_heading level0 row10\" >yeah</th>\n",
       "      <td id=\"T_191de_row10_col0\" class=\"data row10 col0\" >nan</td>\n",
       "      <td id=\"T_191de_row10_col1\" class=\"data row10 col1\" >0.039755</td>\n",
       "      <td id=\"T_191de_row10_col2\" class=\"data row10 col2\" >nan</td>\n",
       "      <td id=\"T_191de_row10_col3\" class=\"data row10 col3\" >0.033254</td>\n",
       "      <td id=\"T_191de_row10_col4\" class=\"data row10 col4\" >0.035709</td>\n",
       "      <td id=\"T_191de_row10_col5\" class=\"data row10 col5\" >0.033467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_191de_level0_row11\" class=\"row_heading level0 row11\" >and</th>\n",
       "      <td id=\"T_191de_row11_col0\" class=\"data row11 col0\" >nan</td>\n",
       "      <td id=\"T_191de_row11_col1\" class=\"data row11 col1\" >nan</td>\n",
       "      <td id=\"T_191de_row11_col2\" class=\"data row11 col2\" >0.024679</td>\n",
       "      <td id=\"T_191de_row11_col3\" class=\"data row11 col3\" >nan</td>\n",
       "      <td id=\"T_191de_row11_col4\" class=\"data row11 col4\" >nan</td>\n",
       "      <td id=\"T_191de_row11_col5\" class=\"data row11 col5\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7d753d4400>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "lines_by_character = {character: [] for character in characters}\n",
    "\n",
    "for season in seasons:\n",
    "    for episode in season['episodes']:\n",
    "        for scene in episode['scenes']:\n",
    "            for utterance in scene['utterances']:\n",
    "                if utterance['speakers'] != []:\n",
    "                    for speaker in utterance['speakers']:\n",
    "                        if speaker in characters:\n",
    "                            lines_by_character[speaker].append(utterance['transcript'])\n",
    "\n",
    "# Create a DataFrame containing the character-specific n-grams\n",
    "ngrams_by_character = {}\n",
    "\n",
    "for character in characters:\n",
    "    # Convert the character's lines into a matrix of TF-IDF scores\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=1000)\n",
    "    X = vectorizer.fit_transform(lines_by_character[character])\n",
    "    # Convert the matrix into a DataFrame and label the columns with the n-grams\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    # Calculate the mean TF-IDF score for each n-gram across all of the character's lines\n",
    "    mean_tfidf_df = pd.DataFrame(tfidf_df.mean(axis=0), columns=['mean_tfidf_score'])\n",
    "    # Remove n-grams that are also common among the other characters\n",
    "    tfidf_df_other = pd.concat([pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()) for other_character in characters if other_character != character], axis=0)\n",
    "    mean_tfidf_df = mean_tfidf_df.drop(columns=tfidf_df_other.columns, errors='ignore')\n",
    "    # Sort the n-grams by their mean TF-IDF score and keep only the top 10 most character-specific n-grams\n",
    "    character_specific_ngrams = mean_tfidf_df.sort_values('mean_tfidf_score', ascending=False).head(10)\n",
    "    ngrams_by_character[character] = character_specific_ngrams\n",
    "\n",
    "# Convert the dictionary into a single DataFrame\n",
    "result_df = pd.concat(ngrams_by_character, axis=1)\n",
    "result_df.columns.names = ['Character', 'N-gram']\n",
    "\n",
    "result_df.style\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabela de n-grams com tf-idf aplicado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monica Geller specific n-grams:\n",
      "                       mean_tfidf_score\n",
      "what are you doing             0.004477\n",
      "are you talking about          0.001861\n",
      "you don have to                0.001551\n",
      "what are you talking           0.001540\n",
      "why don you just               0.001484\n",
      "oh my god you                  0.001359\n",
      "what do you think              0.001345\n",
      "oh my god that                 0.001242\n",
      "do you want to                 0.001132\n",
      "no no no no                    0.001124\n",
      "Joey Tribbiani specific n-grams:\n",
      "                     mean_tfidf_score\n",
      "no no no no                  0.003582\n",
      "what are you doing           0.002219\n",
      "days of our lives            0.002029\n",
      "oh no no no                  0.001993\n",
      "all right all right          0.001483\n",
      "what do you think            0.001044\n",
      "what do you say              0.000995\n",
      "you don have to              0.000972\n",
      "you want me to               0.000894\n",
      "hey hey hey hey              0.000873\n",
      "Chandler Bing specific n-grams:\n",
      "                    mean_tfidf_score\n",
      "no no no no                 0.004155\n",
      "oh no no no                 0.002703\n",
      "what are you doing          0.002406\n",
      "you don have to             0.001436\n",
      "you want me to              0.000951\n",
      "what do you think           0.000829\n",
      "you re gonna be             0.000814\n",
      "it gonna be okay            0.000778\n",
      "do you have any             0.000700\n",
      "how do you know             0.000686\n",
      "Phoebe Buffay specific n-grams:\n",
      "                    mean_tfidf_score\n",
      "what are you doing          0.001839\n",
      "you don have to             0.001605\n",
      "oh my god you               0.001495\n",
      "la la la la                 0.001433\n",
      "do you want to              0.001350\n",
      "oh no no no                 0.001128\n",
      "no no no no                 0.001123\n",
      "you re you re               0.001046\n",
      "you re gonna be             0.000851\n",
      "what are you gonna          0.000837\n",
      "Ross Geller specific n-grams:\n",
      "                    mean_tfidf_score\n",
      "what are you doing          0.003220\n",
      "oh no no no                 0.001516\n",
      "no no no no                 0.001450\n",
      "what do you mean            0.001291\n",
      "you re you re               0.001255\n",
      "what do you think           0.001126\n",
      "it was it was               0.000938\n",
      "come on come on             0.000935\n",
      "know what you re            0.000901\n",
      "you re gonna be             0.000892\n",
      "Rachel Green specific n-grams:\n",
      "                     mean_tfidf_score\n",
      "no no no no                  0.003011\n",
      "what are you doing           0.002702\n",
      "oh no no no                  0.001447\n",
      "oh my god you                0.001337\n",
      "oh my god oh                 0.001299\n",
      "why don you just             0.001237\n",
      "what do you mean             0.001030\n",
      "you want me to               0.001016\n",
      "thank you thank you          0.000989\n",
      "are you doing here           0.000959\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Loop through each character and identify their character-specific n-grams\n",
    "for character in characters:\n",
    "    # Filter the DataFrame to only include the character's lines\n",
    "    character_lines = []\n",
    "\n",
    "    for season in seasons:\n",
    "        for episode in season['episodes']:\n",
    "            for scene in episode['scenes']:\n",
    "                for utterance in scene['utterances']:\n",
    "                    if utterance['speakers'] != []:\n",
    "                        if character in utterance['speakers']:\n",
    "                            character_lines.append(utterance['transcript'])\n",
    "\n",
    "\n",
    "    # Create a TfidfVectorizer object to convert the text into a matrix of TF-IDF scores\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(4, 4), max_features=1000)\n",
    "    X = vectorizer.fit_transform(character_lines)\n",
    "\n",
    "    # Convert the matrix into a DataFrame and label the columns with the n-grams\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Calculate the mean TF-IDF score for each n-gram across all of the character's lines\n",
    "    mean_tfidf_df = pd.DataFrame(tfidf_df.mean(axis=0), columns=['mean_tfidf_score'])\n",
    "\n",
    "    # Sort the n-grams by their mean TF-IDF score and keep only the top 10 most character-specific n-grams\n",
    "    character_specific_ngrams = mean_tfidf_df.sort_values('mean_tfidf_score', ascending=False).head(10)\n",
    "\n",
    "    print(f'{character} specific n-grams:')\n",
    "    print(character_specific_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['get it hi' 'it hi tim' 'debra winger had' ... 'are you doing'\n",
      "  'what are you' 'oh my god']]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "n_gram = 3\n",
    "\n",
    "# Calculate tf-idf scores for all n-grams in the utterances\n",
    "vectorizer = TfidfVectorizer(ngram_range=(n_gram, n_gram))\n",
    "tfidf = vectorizer.fit(all_lines)\n",
    "\n",
    "\n",
    "# Transform the dialogue for each character into a TF-IDF matrix\n",
    "monica_tfidf = tfidf.transform(monica_lines)\n",
    "ross_tfidf = tfidf.transform(ross_lines)\n",
    "chandler_tfidf = tfidf.transform(chandler_lines)\n",
    "joey_tfidf = tfidf.transform(joey_lines)\n",
    "rachel_tfidf = tfidf.transform(rachel_lines)\n",
    "\n",
    "# Get the feature names (i.e., the n-grams) from the TfidfVectorizer\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Define a function to get the top n-grams for each character\n",
    "def get_top_ngrams(tfidf_matrix, character_name, n):\n",
    "    # Convert the TF-IDF matrix to a dense array\n",
    "    dense_matrix = tfidf_matrix.toarray()\n",
    "    # Get the row index for the character in the TF-IDF matrix\n",
    "    character_index = all_dialogue.index(character_name)\n",
    "    # Get the TF-IDF scores for the character's dialogue\n",
    "    character_scores = dense_matrix[character_index]\n",
    "    # Get the indices of the top n scores\n",
    "    top_indices = character_scores.argsort()[-n:][::-1]\n",
    "    # Get the corresponding n-gram strings and scores\n",
    "    top_ngrams = [(feature_names[i], character_scores[i]) for i in top_indices]\n",
    "    return top_ngrams\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
