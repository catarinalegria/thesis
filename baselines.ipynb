{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/catarina/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "characters = ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green', 'others']\n",
    "\n",
    "#test set 1 = season 8\n",
    "\n",
    "with open('sets/test_set1.json') as f:\n",
    "    test_set1 = json.load(f)\n",
    "\n",
    "with open('sets/train_set1.json') as f:\n",
    "    train_set1 = json.load(f)\n",
    "\n",
    "################################################################\n",
    "\n",
    "#test set 2 = 10% of each season\n",
    "\n",
    "with open('sets/test_set2.json') as f:\n",
    "    test_set2 = json.load(f)\n",
    "\n",
    "with open('sets/train_set2.json') as f:\n",
    "    train_set2 = json.load(f)\n",
    "\n",
    "\n",
    "#CHANGE HERE\n",
    "test_set = test_set2\n",
    "train_set = train_set2\n",
    "\n",
    "all_lines = []\n",
    "\n",
    "#corpus\n",
    "for line in train_set:\n",
    "    all_lines.append(line['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit(all_lines)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "test_lines = []\n",
    "speakers = []\n",
    "\n",
    "for line in test_set:\n",
    "    test_lines.append(line['transcript'])\n",
    "    speakers.append(line['speakers'][0])\n",
    "\n",
    "tf_idf = vectorizer.transform(test_lines)\n",
    "\n",
    "dense = tf_idf.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "values = []\n",
    "\n",
    "for sentence in dense:\n",
    "    values.append(sentence.sum())\n",
    "\n",
    "\n",
    "#associate values with sentences\n",
    "sentences_tfidf = []\n",
    "for i in range(len(test_lines)):\n",
    "    sentences_tfidf.append({'sentence': test_lines[i], 'value': values[i], 'speaker': speakers[i]})\n",
    "\n",
    "#sort sentences by value\n",
    "sentences_tfidf = sorted(sentences_tfidf, key = lambda i: i['value'], reverse=True)\n",
    "\n",
    "sentences_only = [sentence['sentence'] for sentence in sentences_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in characters:\n",
    "    character_lines = []\n",
    "    for line in train_set:\n",
    "        if character != 'others' and character in line['speakers']:\n",
    "            character_lines.append(line['transcript'])\n",
    "        elif character == 'others' and line['speakers'][0] not in characters:\n",
    "            character_lines.append(line['transcript'])\n",
    "\n",
    "    #change x for number of lines \n",
    "    values = [2000]\n",
    "\n",
    "    for x in values:\n",
    "\n",
    "        randomlist = random.sample(character_lines, x)\n",
    "\n",
    "        f = open(\"embeddings\" + \"/\" + character + str(x)+ \".txt\", \"w\")\n",
    "        for line in randomlist:\n",
    "            f.write(line+\"\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(vector):\n",
    "\n",
    "    angles = {}\n",
    "\n",
    "    for character in characters:\n",
    "        angles[character] = cosine_similarity(vector.reshape(1,-1), embeddings[character].reshape(1,-1))[0][0]\n",
    "\n",
    "    \n",
    "    \n",
    "    #Smaller angles between vectors produce larger cosine values, indicating greater cosine similarity\n",
    "\n",
    "    character = [i for i in angles if angles[i]==max(angles.values())]\n",
    "\n",
    "\n",
    "    return character[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size embedding 2000\n",
      "20.133111480865225\n",
      "accuracy:  21.686078757626177\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#change x for number of lines \n",
    "#values = [5,10,20,50,100,150,200,500,1000,2000,5000]\n",
    "#values = [2000]\n",
    "\n",
    "values = [2000]\n",
    "\n",
    "for value in values:\n",
    "    sentences = []\n",
    "\n",
    "    for character in characters:\n",
    "        f = open(\"embeddings\" + \"/\" + character + str(value) + \".txt\", \"r\")\n",
    "        lines = f.readlines()\n",
    "        sentences.append(lines)\n",
    "\n",
    "    i=0\n",
    "    embeddings = {}\n",
    "    for character in characters:\n",
    "        embeddings[character] = model.encode(sentences[i])\n",
    "        i+=1\n",
    "\n",
    "    #to confirm number of lines used to create the embedding\n",
    "    print(\"size embedding\", len(embeddings['Monica Geller']))\n",
    "\n",
    "    for character in characters:\n",
    "\n",
    "        mean = embeddings[character][0]\n",
    "        for embedding in embeddings[character]:\n",
    "            mean += embedding\n",
    "        \n",
    "        embeddings[character] = mean\n",
    "    \n",
    "\n",
    "    #####test set computation\n",
    "    \n",
    "    predicted = []\n",
    "    real = []\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for utterance in test_set:\n",
    "        #filtration\n",
    "        index = sentences_only.index(utterance['transcript'])\n",
    "        if sentences_tfidf[index]['value'] >= 3:\n",
    "            if len(utterance['speakers']) == 1:\n",
    "\n",
    "                total += 1\n",
    "\n",
    "                line_embed = model.encode(utterance['transcript'])\n",
    "            \n",
    "                pred = compute_similarity(line_embed)\n",
    "\n",
    "                predicted.append(pred)\n",
    "                real.append(utterance['speakers'][0])\n",
    "\n",
    "                if pred == utterance['speakers'][0]:\n",
    "                    correct+=1\n",
    "                elif pred == \"others\" and utterance['speakers'][0] not in ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green']:\n",
    "                    correct+=1\n",
    "\n",
    "                        \n",
    "    print(accuracy_score(real, predicted)*100)\n",
    "\n",
    "    accuracy = (float(correct)/total)*100\n",
    "\n",
    "    print(\"accuracy: \", accuracy)\n",
    "\n",
    "    #print(\"f1 score: \", f1_score(real, predicted, average='macro'))\n",
    "\n",
    "    print(len(characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning: Nayve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit(all_lines)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "test_lines = []\n",
    "speakers = []\n",
    "\n",
    "for line in test_set:\n",
    "    test_lines.append(line['transcript'])\n",
    "    speakers.append(line['speakers'][0])\n",
    "\n",
    "tf_idf = vectorizer.transform(test_lines)\n",
    "\n",
    "dense = tf_idf.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "values = []\n",
    "\n",
    "for sentence in dense:\n",
    "    values.append(sentence.sum())\n",
    "\n",
    "\n",
    "#associate values with sentences\n",
    "sentences_tfidf = []\n",
    "for i in range(len(test_lines)):\n",
    "    sentences_tfidf.append({'sentence': test_lines[i], 'value': values[i], 'speaker': speakers[i]})\n",
    "\n",
    "#sort sentences by value\n",
    "sentences_tfidf = sorted(sentences_tfidf, key = lambda i: i['value'], reverse=True)\n",
    "\n",
    "sentences_only = [sentence['sentence'] for sentence in sentences_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 31.45%\n"
     ]
    }
   ],
   "source": [
    "train_set_lines = []\n",
    "train_set_speakers = []\n",
    "\n",
    "for utterance in train_set:\n",
    "    #ensure it is NOT a scene annotation and it is an actual line\n",
    "    if utterance['speakers']!= []:\n",
    "        train_set_lines.append(utterance['transcript'])\n",
    "        train_set_speakers.append(utterance['speakers'][0])\n",
    "\n",
    "\n",
    "test_set_lines = []\n",
    "test_set_speakers = []\n",
    "\n",
    "for utterance in test_set:\n",
    "    #filtration\n",
    "    index = sentences_only.index(utterance['transcript'])\n",
    "    if sentences_tfidf[index]['value'] >= 3:\n",
    "        #ensure it is NOT a scene annotation and it is an actual line\n",
    "        if utterance['speakers']!= []:\n",
    "            test_set_lines.append(utterance['transcript'])\n",
    "            test_set_speakers.append(utterance['speakers'][0])\n",
    "\n",
    "# Word Counts\n",
    "vectorizer = CountVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(train_set_lines)\n",
    "test_vectors = vectorizer.transform(test_set_lines)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_vectors, train_set_speakers)\n",
    "\n",
    "# Classify the test set\n",
    "test_predictions = clf.predict(test_vectors)\n",
    "\n",
    "accuracy = accuracy_score(test_set_speakers, test_predictions)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/catarina/miniconda3/envs/advisor/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 1.88MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.41MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 482/482 [00:00<00:00, 1.22MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 1.42G/1.42G [05:55<00:00, 4.00MB/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load the trained RoBERTa model and tokenizer\n",
    "model_name = \"roberta-large\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load your test set JSON\n",
    "with open('sets/train_set1.json', 'r') as json_file:\n",
    "    test_data = json.load(json_file)\n",
    "\n",
    "# Define a function to predict the speaker for a given transcript\n",
    "def predict_speaker(transcript):\n",
    "    input_ids = tokenizer.encode(transcript, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids).logits\n",
    "    predicted_class = logits.argmax().item()\n",
    "\n",
    "    # Map predicted_class to speaker names or \"others\"\n",
    "    speakers = ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green', 'others']\n",
    "    predicted_speaker = speakers[predicted_class]\n",
    "    \n",
    "    return predicted_speaker\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = []\n",
    "\n",
    "for data_point in test_data:\n",
    "    transcript = data_point['transcript']\n",
    "    predicted_speaker = predict_speaker(transcript)\n",
    "    predictions.append({\"transcript\": transcript, \"predicted_speaker\": predicted_speaker})\n",
    "\n",
    "# Save the predictions to a JSON file\n",
    "with open('predictions.json', 'w') as json_file:\n",
    "    json.dump(predictions, json_file, indent=4)\n",
    "\n",
    "print(\"Predictions saved to 'predictions.json'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
