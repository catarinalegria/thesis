{"cells":[{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["import re\n","import csv\n","import string\n","import operator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","#nltk.download('all')\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from collections import Counter"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Auxiliar functions\n","- remove_scenes_actions\n","- generate_ngrams\n","- build_dict"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def remove_scenes_actions(line):\n","    new = \"\"\n","    found = False\n","    for letter in line:\n","        if letter == \"(\" or letter ==\"[\":\n","            found = True\n","        if letter == \")\" or letter ==\"]\":\n","            found = False\n","\n","        #outside brackets\n","        if found == False and letter!=\")\" and letter!=\"]\":\n","            new = new + letter\n","\n","    \n","    return(new)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def generate_ngrams(sentence: str, n: int):\n","    # Convert sentence to lowercase and remove punctuation\n","    import string\n","    sentence = sentence.lower().translate(str.maketrans('', '', string.punctuation))\n","\n","    # Split sentence into words\n","    words = sentence.split()\n","\n","    # Generate N-grams\n","    ngrams = []\n","    for i in range(len(words) - n + 1):\n","        ngrams.append(' '.join(words[i:i+n]))\n","\n","    return ngrams"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["def build_dict(data, characters,n):\n","    values = []\n","\n","    for character in characters:\n","            lines = []\n","\n","            for row in data:\n","                if row[-2] == character:\n","                    #we only want dialogue\n","                    str = remove_scenes_actions(row[-1])\n","                    lines.append(str)\n","\n","            n_grams = []\n","\n","            for line in lines:\n","                #remve punctuation\n","                line = line.translate(str.maketrans('', '', string.punctuation))\n","\n","                n_gram = generate_ngrams(line,n)\n","                for word in n_gram:\n","                    n_grams.append(word)\n","\n","            counter = {}\n","            for elem in n_grams:\n","                if elem in counter:\n","                    counter[elem]+=1\n","                else:\n","                    counter[elem]=1\n","\n","            \n","            counter_sorted = sorted(counter.items(), key=operator.itemgetter(1))\n","            counts = counter_sorted[::-1]\n","\n","            values.append(counts)\n","\n","    dict = {key: value for key, value in zip(characters, values)}\n","\n","    return dict"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["file = open(\"friends.csv\", 'r', encoding=\"utf8\")\n","csvreader = csv.reader(file)\n","data = list(csvreader)\n","\n","characters =[\"Rachel\", \"Monica\", \"Joey\", \"Chandler\", \"Ross\", \"Phoebe\"]\n","\n","stop_words = set(stopwords.words('english'))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["UNIGRAMS"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["values = []\n","\n","for character in characters:\n","        lines = []\n","        counts = []\n","        words = []\n","\n","        for row in data:\n","            if row[-2] == character:\n","                #we only want dialogue\n","                str = remove_scenes_actions(row[-1])\n","                lines.append(str)\n","\n","        for line in lines:\n","            #remve punctuation\n","            line = line.translate(str.maketrans('', '', string.punctuation))\n","            #sp_character.append(line)\n","            \n","            line = word_tokenize(line)\n","\n","            for word in line:\n","                if word not in stop_words and word != \"â€™\":\n","                    words.append(word.lower())\n","\n","        counts = Counter(words)\n","        counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n","\n","        values.append(counts)\n","\n","dict = {key: value for key, value in zip(characters, values)}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Transform the array *counts* to a dataframe and output it "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","df = pd.DataFrame.from_dict(dict, orient='index')\n","\n","# displaying the DataFrame\n","df.style\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["BIGRAMS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dict = build_dict(data, characters, 2)\n","\n","df = pd.DataFrame.from_dict(dict, orient='index')\n","\n","# displaying the DataFrame\n","df.style"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["TRIGRAMS "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dict = build_dict(data, characters, 3)\n","\n","df = pd.DataFrame.from_dict(dict, orient='index')\n","\n","# displaying the DataFrame\n","df.style"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["4-GRAMS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dict = build_dict(data, characters, 4)\n","\n","df = pd.DataFrame.from_dict(dict, orient='index')\n","\n","# displaying the DataFrame\n","df.style"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["5-GRAMS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dict = build_dict(data, characters, 5)\n","\n","df = pd.DataFrame.from_dict(dict, orient='index')\n","\n","# displaying the DataFrame\n","df.style"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["TF-IDF of the n-grams"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","for character in characters:\n","    lines = []\n","    sp_character = []\n","    \n","    for row in data:\n","        if row[-2] == character:\n","            #we only want dialogue\n","            str = remove_scenes_actions(row[-1])\n","\n","            lines.append(str)\n","\n","    ngram_range = (1, 4)\n","\n","    # Define a custom tokenizer that only includes alphanumeric characters\n","    def tokenizer_new(text):\n","        # Convert to lowercase and split into words\n","        words = text.lower().split()\n","        # Remove punctuation and return only alphanumeric characters\n","        return [re.sub(r'\\W+', '', w) for w in words]\n","\n","    # Tokenize the texts into individual words and n-grams\n","    tokenizer = nltk.tokenize.word_tokenize\n","    tfidf = TfidfVectorizer(stop_words='english', tokenizer=tokenizer_new, ngram_range=ngram_range)\n","\n","    # Compute the TF-IDF scores for all the n-grams in all the texts combined\n","    tfidf_scores = tfidf.fit_transform(lines).toarray()[0]\n","\n","    # Get the n-gram vocabulary and their corresponding indices in the tfidf_scores array\n","    ngram_vocab = tfidf.get_feature_names_out()\n","\n","    # Print the TF-IDF scores for each n-gram in descending order\n","    #for ngram, score in sorted(zip(ngram_vocab, tfidf_scores), key=lambda x: x[1], reverse=True):\n","    #    print(\"{}: {}\".format(ngram, score))\n","\n","\n","    # Create a pandas DataFrame to store the n-grams and their corresponding TF-IDF scores\n","    df = pd.DataFrame({'ngram': ngram_vocab, 'tfidf': tfidf_scores})\n","\n","    # Sort the DataFrame by TF-IDF score in descending order\n","    df = df.sort_values(by='tfidf', ascending=False)\n","\n","    # Print the DataFrame\n","    print(character)\n","    print(df)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":2}
