{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /cfs/home/u021320/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import operator\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def generate_ngrams(sentence, n):\n",
    "    # Convert sentence to lowercase and remove punctuation\n",
    "    import string\n",
    "    sentence = sentence.lower().translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\n",
    "    # Split sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Remove stop words\n",
    "\n",
    "    #words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    #words = [PorterStemmer().stem(w) for w in words]\n",
    "\n",
    "    # Generate N-grams\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngrams.append(' '.join(words[i:i+n]))\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(train_set, characters,n):\n",
    "    values = []\n",
    "    \n",
    "    for character in characters:\n",
    "            lines = []\n",
    "            counts = []\n",
    "            words = []\n",
    "\n",
    "            for utterance in train_set:\n",
    "                if character in utterance['speakers'] and utterance['transcript'] != \"\":\n",
    "                    lines.append(utterance['transcript'])\n",
    "                elif character == 'Others' and utterance['speakers'][0] not in characters and utterance['transcript'] != \"\":\n",
    "                    lines.append(utterance['transcript'])\n",
    "\n",
    "            n_grams = []\n",
    "\n",
    "            for line in lines:\n",
    "                #remve punctuation\n",
    "                line = line.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\n",
    "                if n==1:\n",
    "                    line = word_tokenize(line)\n",
    "\n",
    "                    for word in line:\n",
    "                        if word not in stop_words and word != \"â€™\":\n",
    "                            words.append(word.lower())    \n",
    "                \n",
    "                else:\n",
    "                    n_gram = generate_ngrams(line,n)\n",
    "                    for word in n_gram:\n",
    "                        n_grams.append(word)\n",
    "\n",
    "\n",
    "            if n==1:\n",
    "                counts = Counter(words)\n",
    "                counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            else:\n",
    "                counter = {}\n",
    "                for elem in n_grams:\n",
    "                    if elem in counter:\n",
    "                        counter[elem]+=1\n",
    "                    else:\n",
    "                        counter[elem]=1\n",
    "                counter_sorted = sorted(counter.items(), key=operator.itemgetter(1))\n",
    "                counts = counter_sorted[::-1]\n",
    "\n",
    "            values.append(counts)\n",
    "\n",
    "    dict = {key: value for key, value in zip(characters, values)}\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sets/train_set1.json') as f:\n",
    "    train_set1 = json.load(f)\n",
    "\n",
    "with open('sets/train_set2.json') as f:\n",
    "    train_set2 = json.load(f)\n",
    "\n",
    "with open('sets/test_set1.json') as f:\n",
    "    test_set1 = json.load(f)\n",
    "\n",
    "with open('sets/test_set2.json') as f:\n",
    "    test_set2 = json.load(f)\n",
    "\n",
    "\n",
    "#CHANGE HERE\n",
    "test_set = test_set2\n",
    "train_set = train_set2\n",
    "\n",
    "\n",
    "#we can do this manually\n",
    "characters = ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green', 'Others']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_68681\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_68681_level0_col0\" class=\"col_heading level0 col0\" >Monica Geller</th>\n",
       "      <th id=\"T_68681_level0_col1\" class=\"col_heading level0 col1\" >Joey Tribbiani</th>\n",
       "      <th id=\"T_68681_level0_col2\" class=\"col_heading level0 col2\" >Chandler Bing</th>\n",
       "      <th id=\"T_68681_level0_col3\" class=\"col_heading level0 col3\" >Phoebe Buffay</th>\n",
       "      <th id=\"T_68681_level0_col4\" class=\"col_heading level0 col4\" >Ross Geller</th>\n",
       "      <th id=\"T_68681_level0_col5\" class=\"col_heading level0 col5\" >Rachel Green</th>\n",
       "      <th id=\"T_68681_level0_col6\" class=\"col_heading level0 col6\" >Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_68681_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_68681_row0_col0\" class=\"data row0 col0\" >('get it ill get it ill get it', 12)</td>\n",
       "      <td id=\"T_68681_row0_col1\" class=\"data row0 col1\" >('i am there i am there i am', 6)</td>\n",
       "      <td id=\"T_68681_row0_col2\" class=\"data row0 col2\" >('no no no no no no no no', 6)</td>\n",
       "      <td id=\"T_68681_row0_col3\" class=\"data row0 col3\" >('you cry and you cry and you cry', 4)</td>\n",
       "      <td id=\"T_68681_row0_col4\" class=\"data row0 col4\" >('ago there were these people called the maccabees', 4)</td>\n",
       "      <td id=\"T_68681_row0_col5\" class=\"data row0 col5\" >('ow ow ow ow ow ow ow ow', 9)</td>\n",
       "      <td id=\"T_68681_row0_col6\" class=\"data row0 col6\" >('confident woman who does not need to smoke', 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_68681_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_68681_row1_col0\" class=\"data row1 col0\" >('ill get it ill get it ill get', 12)</td>\n",
       "      <td id=\"T_68681_row1_col1\" class=\"data row1 col1\" >('there i am there i am there i', 6)</td>\n",
       "      <td id=\"T_68681_row1_col2\" class=\"data row1 col2\" >('ow ow ow ow ow ow ow ow', 5)</td>\n",
       "      <td id=\"T_68681_row1_col3\" class=\"data row1 col3\" >('cat smelly cat what are they feeding you', 3)</td>\n",
       "      <td id=\"T_68681_row1_col4\" class=\"data row1 col4\" >('years ago there were these people called the', 4)</td>\n",
       "      <td id=\"T_68681_row1_col5\" class=\"data row1 col5\" >('my god oh my god oh my god', 6)</td>\n",
       "      <td id=\"T_68681_row1_col6\" class=\"data row1 col6\" >('strong confident woman who does not need to', 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_68681_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_68681_row2_col0\" class=\"data row2 col0\" >('it ill get it ill get it ill', 11)</td>\n",
       "      <td id=\"T_68681_row2_col1\" class=\"data row2 col1\" >('can i talk to you for a second', 5)</td>\n",
       "      <td id=\"T_68681_row2_col2\" class=\"data row2 col2\" >('good game good game good game good game', 4)</td>\n",
       "      <td id=\"T_68681_row2_col3\" class=\"data row2 col3\" >('smelly cat smelly cat what are they feeding', 3)</td>\n",
       "      <td id=\"T_68681_row2_col4\" class=\"data row2 col4\" >('and years ago there were these people called', 4)</td>\n",
       "      <td id=\"T_68681_row2_col5\" class=\"data row2 col5\" >('oh my god oh my god oh my', 6)</td>\n",
       "      <td id=\"T_68681_row2_col6\" class=\"data row2 col6\" >('a strong confident woman who does not need', 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_68681_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_68681_row3_col0\" class=\"data row3 col0\" >('no no no no no no no no', 4)</td>\n",
       "      <td id=\"T_68681_row3_col1\" class=\"data row3 col1\" >('am there i am there i am there', 4)</td>\n",
       "      <td id=\"T_68681_row3_col2\" class=\"data row3 col2\" >('get out get out get out get out', 4)</td>\n",
       "      <td id=\"T_68681_row3_col3\" class=\"data row3 col3\" >('cat what are they feeding you smelly cat', 3)</td>\n",
       "      <td id=\"T_68681_row3_col4\" class=\"data row3 col4\" >('years and years ago there were these people', 4)</td>\n",
       "      <td id=\"T_68681_row3_col5\" class=\"data row3 col5\" >('god oh my god oh my god oh', 5)</td>\n",
       "      <td id=\"T_68681_row3_col6\" class=\"data row3 col6\" >('see whats going on here this man is', 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_68681_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_68681_row4_col0\" class=\"data row4 col0\" >('why wont i be married when im 40', 4)</td>\n",
       "      <td id=\"T_68681_row4_col1\" class=\"data row4 col1\" >('quack tweet tweet quack quack tweet tweet quack', 3)</td>\n",
       "      <td id=\"T_68681_row4_col2\" class=\"data row4 col2\" >('game good game good game good game good', 3)</td>\n",
       "      <td id=\"T_68681_row4_col3\" class=\"data row4 col3\" >('cat smellly cat what are they feeding you', 3)</td>\n",
       "      <td id=\"T_68681_row4_col4\" class=\"data row4 col4\" >('lets cool off okay lets get some frozen', 3)</td>\n",
       "      <td id=\"T_68681_row4_col5\" class=\"data row4 col5\" >('i am yes i am yes i am', 4)</td>\n",
       "      <td id=\"T_68681_row4_col6\" class=\"data row4 col6\" >('you see whats going on here this man', 4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4d68eb3c10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change 3rd argument to 1 for unigrams, 2 for bigrams, 3 for trigrams etc\n",
    "dict = build_dict(train_set2, characters, 8)\n",
    "\n",
    "df = pd.DataFrame.from_dict(dict, orient='index')\n",
    "\n",
    "df_new = df.iloc[:, 0:5]\n",
    "\n",
    "# displaying the DataFrame\n",
    "df_new.T.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(profiles_character, profile_utterance, characters):\n",
    "    similarities = []\n",
    "    for profile_character in profiles_character:\n",
    "        similarity = 0\n",
    "        matches = 0\n",
    "        for ngram in profile_character:\n",
    "            # only common n-grams are taken into account\n",
    "            if ngram in profile_utterance:\n",
    "                matches += 1\n",
    "                # similarity = (2*(profile_character[ngram] - profile_utterance[ngram])/ (profile_character[ngram] + profile_utterance[ngram]))Â²\n",
    "                similarity += ((2*(profile_character[ngram] - profile_utterance[ngram])) / (profile_character[ngram] + profile_utterance[ngram]))**2\n",
    "        \n",
    "        if matches != 0:\n",
    "            similarities.append(similarity)\n",
    "        else:\n",
    "            # there are no common N-grams (intersection is empty)\n",
    "            similarities.append(None)\n",
    "\n",
    "    if similarities == [None,None,None,None,None,None,None]:\n",
    "        return None\n",
    "\n",
    "    return characters[similarities.index(min([x for x in similarities if x is not None]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_profile_utterance(utterance, n):\n",
    "    n_grams = []\n",
    "\n",
    "    utterance = utterance.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\n",
    "    n_gram = generate_ngrams(utterance,n)\n",
    "    \n",
    "    for word in n_gram:\n",
    "        n_grams.append(word)\n",
    "        \n",
    "    counter = {}\n",
    "    for elem in n_grams:\n",
    "        if elem in counter:\n",
    "            counter[elem]+=1\n",
    "        else:\n",
    "            counter[elem]=1\n",
    "    \n",
    "    counter_sorted = sorted(counter.items(), key=operator.itemgetter(1))\n",
    "    counts = counter_sorted[::-1]\n",
    "\n",
    "    return {key: value/len(counts) for key, value in counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile size (M): 5\n",
      "n-gram size (N): 2\n",
      "correct:  122\n",
      "total:  622\n",
      "accuracy:  19.614147909967848\n",
      "n-gram size (N): 3\n",
      "correct:  38\n",
      "total:  168\n",
      "accuracy:  22.61904761904762\n",
      "n-gram size (N): 4\n",
      "correct:  16\n",
      "total:  61\n",
      "accuracy:  26.229508196721312\n",
      "n-gram size (N): 5\n",
      "correct:  11\n",
      "total:  25\n",
      "accuracy:  44.0\n",
      "n-gram size (N): 6\n",
      "correct:  4\n",
      "total:  8\n",
      "accuracy:  50.0\n",
      "\n",
      "\n",
      "profile size (M): 10\n",
      "n-gram size (N): 2\n",
      "correct:  208\n",
      "total:  1113\n",
      "accuracy:  18.68823000898473\n",
      "n-gram size (N): 3\n",
      "correct:  90\n",
      "total:  381\n",
      "accuracy:  23.62204724409449\n",
      "n-gram size (N): 4\n",
      "correct:  29\n",
      "total:  117\n",
      "accuracy:  24.786324786324787\n",
      "n-gram size (N): 5\n",
      "correct:  11\n",
      "total:  44\n",
      "accuracy:  25.0\n",
      "n-gram size (N): 6\n",
      "correct:  7\n",
      "total:  18\n",
      "accuracy:  38.88888888888889\n",
      "\n",
      "\n",
      "profile size (M): 20\n",
      "n-gram size (N): 2\n",
      "correct:  332\n",
      "total:  2022\n",
      "accuracy:  16.419386745796242\n",
      "n-gram size (N): 3\n",
      "correct:  149\n",
      "total:  666\n",
      "accuracy:  22.372372372372375\n",
      "n-gram size (N): 4\n",
      "correct:  46\n",
      "total:  220\n",
      "accuracy:  20.909090909090907\n",
      "n-gram size (N): 5\n",
      "correct:  15\n",
      "total:  74\n",
      "accuracy:  20.27027027027027\n",
      "n-gram size (N): 6\n",
      "correct:  9\n",
      "total:  27\n",
      "accuracy:  33.33333333333333\n",
      "\n",
      "\n",
      "profile size (M): 50\n",
      "n-gram size (N): 2\n",
      "correct:  448\n",
      "total:  2808\n",
      "accuracy:  15.954415954415953\n",
      "n-gram size (N): 3\n",
      "correct:  235\n",
      "total:  1192\n",
      "accuracy:  19.71476510067114\n",
      "n-gram size (N): 4\n",
      "correct:  89\n",
      "total:  420\n",
      "accuracy:  21.19047619047619\n",
      "n-gram size (N): 5\n",
      "correct:  24\n",
      "total:  129\n",
      "accuracy:  18.6046511627907\n",
      "n-gram size (N): 6\n",
      "correct:  12\n",
      "total:  41\n",
      "accuracy:  29.268292682926827\n",
      "\n",
      "\n",
      "profile size (M): 100\n",
      "n-gram size (N): 2\n",
      "correct:  534\n",
      "total:  3403\n",
      "accuracy:  15.692036438436674\n",
      "n-gram size (N): 3\n",
      "correct:  299\n",
      "total:  1514\n",
      "accuracy:  19.74900924702774\n",
      "n-gram size (N): 4\n",
      "correct:  113\n",
      "total:  588\n",
      "accuracy:  19.217687074829932\n",
      "n-gram size (N): 5\n",
      "correct:  34\n",
      "total:  174\n",
      "accuracy:  19.54022988505747\n",
      "n-gram size (N): 6\n",
      "correct:  12\n",
      "total:  47\n",
      "accuracy:  25.53191489361702\n",
      "\n",
      "\n",
      "profile size (M): 150\n",
      "n-gram size (N): 2\n",
      "correct:  576\n",
      "total:  3642\n",
      "accuracy:  15.815485996705107\n",
      "n-gram size (N): 3\n",
      "correct:  336\n",
      "total:  1735\n",
      "accuracy:  19.36599423631124\n",
      "n-gram size (N): 4\n",
      "correct:  124\n",
      "total:  680\n",
      "accuracy:  18.235294117647058\n",
      "n-gram size (N): 5\n",
      "correct:  39\n",
      "total:  192\n",
      "accuracy:  20.3125\n",
      "n-gram size (N): 6\n",
      "correct:  12\n",
      "total:  52\n",
      "accuracy:  23.076923076923077\n",
      "\n",
      "\n",
      "profile size (M): 200\n",
      "n-gram size (N): 2\n",
      "correct:  618\n",
      "total:  3850\n",
      "accuracy:  16.051948051948052\n",
      "n-gram size (N): 3\n",
      "correct:  354\n",
      "total:  1922\n",
      "accuracy:  18.41831425598335\n",
      "n-gram size (N): 4\n",
      "correct:  133\n",
      "total:  756\n",
      "accuracy:  17.59259259259259\n",
      "n-gram size (N): 5\n",
      "correct:  48\n",
      "total:  212\n",
      "accuracy:  22.641509433962266\n",
      "n-gram size (N): 6\n",
      "correct:  12\n",
      "total:  55\n",
      "accuracy:  21.818181818181817\n",
      "\n",
      "\n",
      "profile size (M): 500\n",
      "n-gram size (N): 2\n",
      "correct:  667\n",
      "total:  4368\n",
      "accuracy:  15.270146520146522\n",
      "n-gram size (N): 3\n",
      "correct:  406\n",
      "total:  2456\n",
      "accuracy:  16.530944625407166\n",
      "n-gram size (N): 4\n",
      "correct:  187\n",
      "total:  1007\n",
      "accuracy:  18.570009930486595\n",
      "n-gram size (N): 5\n",
      "correct:  61\n",
      "total:  276\n",
      "accuracy:  22.10144927536232\n",
      "n-gram size (N): 6\n",
      "correct:  16\n",
      "total:  66\n",
      "accuracy:  24.242424242424242\n",
      "\n",
      "\n",
      "profile size (M): 1000\n",
      "n-gram size (N): 2\n",
      "correct:  627\n",
      "total:  4661\n",
      "accuracy:  13.452048916541514\n",
      "n-gram size (N): 3\n",
      "correct:  464\n",
      "total:  2870\n",
      "accuracy:  16.16724738675958\n",
      "n-gram size (N): 4\n",
      "correct:  220\n",
      "total:  1189\n",
      "accuracy:  18.502943650126156\n",
      "n-gram size (N): 5\n",
      "correct:  79\n",
      "total:  344\n",
      "accuracy:  22.96511627906977\n",
      "n-gram size (N): 6\n",
      "correct:  24\n",
      "total:  87\n",
      "accuracy:  27.586206896551722\n",
      "\n",
      "\n",
      "profile size (M): 2000\n",
      "n-gram size (N): 2\n",
      "correct:  601\n",
      "total:  4881\n",
      "accuracy:  12.313050604384348\n",
      "n-gram size (N): 3\n",
      "correct:  543\n",
      "total:  3241\n",
      "accuracy:  16.754088244369022\n",
      "n-gram size (N): 4\n",
      "correct:  283\n",
      "total:  1440\n",
      "accuracy:  19.65277777777778\n",
      "n-gram size (N): 5\n",
      "correct:  91\n",
      "total:  402\n",
      "accuracy:  22.63681592039801\n",
      "n-gram size (N): 6\n",
      "correct:  27\n",
      "total:  97\n",
      "accuracy:  27.835051546391753\n",
      "\n",
      "\n",
      "profile size (M): 5000\n",
      "n-gram size (N): 2\n",
      "correct:  666\n",
      "total:  5037\n",
      "accuracy:  13.222156045265038\n",
      "n-gram size (N): 3\n",
      "correct:  550\n",
      "total:  3610\n",
      "accuracy:  15.23545706371191\n",
      "n-gram size (N): 4\n",
      "correct:  319\n",
      "total:  1642\n",
      "accuracy:  19.427527405602923\n",
      "n-gram size (N): 5\n",
      "correct:  110\n",
      "total:  476\n",
      "accuracy:  23.10924369747899\n",
      "n-gram size (N): 6\n",
      "correct:  34\n",
      "total:  120\n",
      "accuracy:  28.333333333333332\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#change x for number of lines \n",
    "values = [5,10,20,50,100,150,200,500,1000,2000,5000]  # profile size\n",
    "#values = [3]\n",
    "n = [2,3,4,5,6]  # n gram size\n",
    "#n = [2]\n",
    "\n",
    "for value in values:\n",
    "\n",
    "    print(\"profile size (M): \" + str(value))\n",
    "\n",
    "    for size in n:\n",
    "        profiles = []\n",
    "        \n",
    "        print(\"n-gram size (N): \" + str(size))\n",
    "        # results for n gram with n=size\n",
    "        dict = build_dict(train_set, characters, size)\n",
    "        for character in dict:\n",
    "            profile = {}\n",
    "            total_n_grams = len(dict[character])\n",
    "\n",
    "            for elem in dict[character][0:value]:\n",
    "                profile[elem[0]] = elem[1]/total_n_grams\n",
    "\n",
    "            profiles.append(profile)\n",
    "        \n",
    "        #####test set computation\n",
    "        \n",
    "        predicted = []\n",
    "        real = []\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for utterance in test_set:\n",
    "            if len(utterance['speakers']) == 1 and utterance['transcript'] != \"\":\n",
    "\n",
    "                profile_utterance = build_profile_utterance(utterance['transcript'], size)\n",
    "\n",
    "                if profile_utterance != {}: # if the utterance is not empty, i.e, has n-grams\n",
    "\n",
    "                    profile_utterance = {k: profile_utterance[k] for k in list(profile_utterance.keys())[:value]}\n",
    "            \n",
    "                    pred = compute_similarity(profiles, profile_utterance, characters)\n",
    "\n",
    "                    if pred is not None:\n",
    "\n",
    "                        total += 1\n",
    "\n",
    "                        predicted.append(pred)\n",
    "                        real.append(utterance['speakers'][0])\n",
    "\n",
    "                        if pred in utterance['speakers']:\n",
    "                            correct+=1\n",
    "                        elif pred == \"Others\" and utterance['speakers'][0] not in ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green']:\n",
    "                            correct+=1\n",
    "\n",
    "        print(\"correct: \", correct)\n",
    "        print(\"total: \", total)\n",
    "        accuracy = (float(correct)/total)*100\n",
    "\n",
    "        print(\"accuracy: \", accuracy)\n",
    "\n",
    "    print('\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
