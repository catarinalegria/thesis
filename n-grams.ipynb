{"cells":[{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["import re\n","import csv\n","import string\n","import operator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","#nltk.download('all')\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from collections import Counter"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Auxiliar functions\n","- remove_scenes_actions\n","- generate_ngrams\n","- build_dict"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def remove_scenes_actions(line):\n","    new = \"\"\n","    found = False\n","    for letter in line:\n","        if letter == \"(\" or letter ==\"[\":\n","            found = True\n","        if letter == \")\" or letter ==\"]\":\n","            found = False\n","\n","        #outside brackets\n","        if found == False and letter!=\")\" and letter!=\"]\":\n","            new = new + letter\n","\n","    \n","    return(new)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def generate_ngrams(sentence: str, n: int):\n","    # Convert sentence to lowercase and remove punctuation\n","    import string\n","    sentence = sentence.lower().translate(str.maketrans('', '', string.punctuation))\n","\n","    # Split sentence into words\n","    words = sentence.split()\n","\n","    # Generate N-grams\n","    ngrams = []\n","    for i in range(len(words) - n + 1):\n","        ngrams.append(' '.join(words[i:i+n]))\n","\n","    return ngrams"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["def build_dict(data, characters,n):\n","    values = []\n","\n","    for character in characters:\n","            lines = []\n","\n","            for row in data:\n","                if row[-2] == character:\n","                    #we only want dialogue\n","                    str = remove_scenes_actions(row[-1])\n","                    lines.append(str)\n","\n","            n_grams = []\n","\n","            for line in lines:\n","                #remve punctuation\n","                line = line.translate(str.maketrans('', '', string.punctuation))\n","\n","                n_gram = generate_ngrams(line,n)\n","                for word in n_gram:\n","                    n_grams.append(word)\n","\n","            counter = {}\n","            for elem in n_grams:\n","                if elem in counter:\n","                    counter[elem]+=1\n","                else:\n","                    counter[elem]=1\n","\n","            \n","            counter_sorted = sorted(counter.items(), key=operator.itemgetter(1))\n","            counts = counter_sorted[::-1]\n","\n","            values.append(counts)\n","\n","    dict = {key: value for key, value in zip(characters, values)}\n","\n","    return dict"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["file = open(\"friends.csv\", 'r', encoding=\"utf8\")\n","csvreader = csv.reader(file)\n","data = list(csvreader)\n","\n","characters =[\"Rachel\", \"Monica\", \"Joey\", \"Chandler\", \"Ross\", \"Phoebe\"]\n","\n","stop_words = set(stopwords.words('english'))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["UNIGRAMS"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["values = []\n","\n","for character in characters:\n","        lines = []\n","        counts = []\n","        words = []\n","\n","        for row in data:\n","            if row[-2] == character:\n","                #we only want dialogue\n","                str = remove_scenes_actions(row[-1])\n","                lines.append(str)\n","\n","        for line in lines:\n","            #remve punctuation\n","            line = line.translate(str.maketrans('', '', string.punctuation))\n","            #sp_character.append(line)\n","            \n","            line = word_tokenize(line)\n","\n","            for word in line:\n","                if word not in stop_words and word != \"â€™\":\n","                    words.append(word.lower())\n","\n","        counts = Counter(words)\n","        counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n","\n","        values.append(counts)\n","\n","dict = {key: value for key, value in zip(characters, values)}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Transform the array *counts* to a dataframe and output it "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","df = pd.DataFrame.from_dict(dict, orient='index')\n","\n","# displaying the DataFrame\n","df.style\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["BIGRAMS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dict = build_dict(data, characters, 2)\n","\n","df = pd.DataFrame.from_dict(dict, orient='index')\n","\n","# displaying the DataFrame\n","df.style"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["TRIGRAMS "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dict = build_dict(data, characters, 3)\n","\n","df = pd.DataFrame.from_dict(dict, orient='index')\n","\n","# displaying the DataFrame\n","df.style"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["4-GRAMS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dict = build_dict(data, characters, 4)\n","\n","df = pd.DataFrame.from_dict(dict, orient='index')\n","\n","# displaying the DataFrame\n","df.style"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["5-GRAMS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dict = build_dict(data, characters, 5)\n","\n","df = pd.DataFrame.from_dict(dict, orient='index')\n","\n","# displaying the DataFrame\n","df.style"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["TF-IDF of the n-grams"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Rachel\n","                                  ngram     tfidf\n","23965                      hi thank god  0.164091\n","17326               god monica hi thank  0.164091\n","23964                          hi thank  0.164091\n","23966                 hi thank god just  0.164091\n","17263            god just went building  0.164091\n","...                                 ...       ...\n","25297  hurely michelle pfieffer dorothy  0.000000\n","25296          hurely michelle pfieffer  0.000000\n","25295                   hurely michelle  0.000000\n","25294                            hurely  0.000000\n","75844            zoo yesterday im koala  0.000000\n","\n","[75845 rows x 2 columns]\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Monica\n","                           ngram    tfidf\n","52441                theres tell  0.27299\n","51221          tell hes just guy  0.27299\n","20581               hes just guy  0.27299\n","52442            theres tell hes  0.27299\n","51220              tell hes just  0.27299\n","...                          ...      ...\n","21136  hey speaking send holiday  0.00000\n","21137                   hey stay  0.00000\n","21138          hey stay chandler  0.00000\n","21139                hey sweetie  0.00000\n","63388           zones got  seven  0.00000\n","\n","[63389 rows x 2 columns]\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Joey\n","                        ngram     tfidf\n","17725        going guy theres  0.244561\n","7840     cmon youre going guy  0.244561\n","7839         cmon youre going  0.244561\n","70946         youre going guy  0.244561\n","70947  youre going guy theres  0.244561\n","...                       ...       ...\n","23900      hey wants ah throw  0.000000\n","23901         hey wants pizza  0.000000\n","23902              hey washed  0.000000\n","23903               hey wasnt  0.000000\n","71656    zygomatic craniotomy  0.000000\n","\n","[71657 rows x 2 columns]\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Chandler\n","                      ngram     tfidf\n","23251                  hump  0.418461\n","23254   hump hump hairpiece  0.218814\n","46440  right joey nice does  0.218814\n","10963             does hump  0.218814\n","46438            right joey  0.218814\n","...                     ...       ...\n","22377  hitlers disappointed  0.000000\n","22378               hitting  0.000000\n","22379         hitting right  0.000000\n","22380    hitting right away  0.000000\n","67113     zorps things said  0.000000\n","\n","[67114 rows x 2 columns]\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Ross\n","                     ngram  tfidf\n","25752                   hi    1.0\n","0                             0.0\n","51883             rach hes    0.0\n","51889  rach ifif want ride    0.0\n","51888       rach ifif want    0.0\n","...                    ...    ...\n","25942     hide oh exciting    0.0\n","25941              hide oh    0.0\n","25940         hide anymore    0.0\n","25939                 hide    0.0\n","77826   zoos yknow like uh    0.0\n","\n","[77827 rows x 2 columns]\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Phoebe\n","                          ngram     tfidf\n","9970                   does eat  0.353919\n","9971             does eat chalk  0.353919\n","11767                 eat chalk  0.353919\n","59026             wait does eat  0.353919\n","59027       wait does eat chalk  0.353919\n","...                         ...       ...\n","21956                huh people  0.000000\n","21957         huh people moving  0.000000\n","21958  huh people moving people  0.000000\n","21959                huh theyre  0.000000\n","65838      Ã©coutez je vais vous  0.000000\n","\n","[65839 rows x 2 columns]\n"]}],"source":["import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","for character in characters:\n","    lines = []\n","    sp_character = []\n","    \n","    for row in data:\n","        if row[-2] == character:\n","            #we only want dialogue\n","            str = remove_scenes_actions(row[-1])\n","\n","            lines.append(str)\n","\n","    ngram_range = (1, 4)\n","\n","    # Define a custom tokenizer that only includes alphanumeric characters\n","    def tokenizer_new(text):\n","        # Convert to lowercase and split into words\n","        words = text.lower().split()\n","        # Remove punctuation and return only alphanumeric characters\n","        return [re.sub(r'\\W+', '', w) for w in words]\n","\n","    # Tokenize the texts into individual words and n-grams\n","    tokenizer = nltk.tokenize.word_tokenize\n","    tfidf = TfidfVectorizer(stop_words='english', tokenizer=tokenizer_new, ngram_range=ngram_range)\n","\n","    # Compute the TF-IDF scores for all the n-grams in all the texts combined\n","    tfidf_scores = tfidf.fit_transform(lines).toarray()[0]\n","\n","    # Get the n-gram vocabulary and their corresponding indices in the tfidf_scores array\n","    ngram_vocab = tfidf.get_feature_names_out()\n","\n","    # Print the TF-IDF scores for each n-gram in descending order\n","    #for ngram, score in sorted(zip(ngram_vocab, tfidf_scores), key=lambda x: x[1], reverse=True):\n","    #    print(\"{}: {}\".format(ngram, score))\n","\n","\n","    # Create a pandas DataFrame to store the n-grams and their corresponding TF-IDF scores\n","    df = pd.DataFrame({'ngram': ngram_vocab, 'tfidf': tfidf_scores})\n","\n","    # Sort the DataFrame by TF-IDF score in descending order\n","    df = df.sort_values(by='tfidf', ascending=False)\n","\n","    # Print the DataFrame\n","    print(character)\n","    print(df)"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":2}
