{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 21:27:11.048778: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-15 21:27:15.578720: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-15 21:27:47.666560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "#from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.out = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self,ids,mask,token_type_ids):\n",
    "        _,o2= self.bert_model(ids,attention_mask=mask,token_type_ids=token_type_ids, return_dict=False)\n",
    "        \n",
    "        out= self.out(o2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FriendsDataset(Dataset):\n",
    "    def __init__(self, data_file, tokenizer):\n",
    "        self.data_file = data_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.utterances, self.labels = self.load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.utterances[idx], self.labels[idx]\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.data_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        utterances = []\n",
    "        labels = []\n",
    "        for utterance in data:\n",
    "            if utterance['speakers'] != []:\n",
    "                utterances.append(utterance['transcript'])\n",
    "\n",
    "                if utterance['speakers'][0] not in  ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green']:\n",
    "                    labels.append(\"Other\")\n",
    "                else:\n",
    "                    labels.append(utterance['speakers'][0])\n",
    "\n",
    "        self.encoder.fit(labels)\n",
    "        labels = self.encoder.transform(labels)\n",
    "\n",
    "        return utterances, labels\n",
    "\n",
    "    def preprocess_data(self, max_sequence_length):\n",
    "        tokenized_utterances = [self.tokenizer.encode(utterance)[:max_sequence_length] for utterance in self.utterances]\n",
    "        padded_utterances = []\n",
    "        for tokenized_utterance in tokenized_utterances:\n",
    "            padded_utterance = list(tokenized_utterance)\n",
    "            if len(padded_utterance) < max_sequence_length:\n",
    "                padded_utterance.extend([0] * (max_sequence_length - len(padded_utterance)))\n",
    "            padded_utterances.append(padded_utterance)\n",
    "\n",
    "        labels = torch.tensor(self.labels)\n",
    "\n",
    "        return padded_utterances, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class Classifier:\n",
    "    def __init__(self, num_classes, embedding_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.sentence_transformer.encode(x)\n",
    "        return self.fc(embeddings)'''\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)#eu sou amiga :)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_layer = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding_layer(x)\n",
    "        embedded = torch.mean(embedded, dim=1)\n",
    "        hidden = self.relu(self.linear_layer(embedded))\n",
    "        output = self.output_layer(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48054\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset from the JSON file\n",
    "with open('sets/test_set1.json') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "# Calculate the vocabulary size\n",
    "vocab = set()\n",
    "for utterance in dataset:\n",
    "    if utterance['speakers'] != []:\n",
    "        words = utterance['transcript'].split()\n",
    "        vocab.update(words)\n",
    "\n",
    "size = len(vocab)\n",
    "\n",
    "# Load the dataset from the JSON file\n",
    "with open('sets/train_set1.json') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "# Calculate the vocabulary size\n",
    "vocab = set()\n",
    "for utterance in dataset:\n",
    "    if utterance['speakers'] != []:\n",
    "        words = utterance['transcript'].split()\n",
    "        vocab.update(words)\n",
    "\n",
    "size2 = len(vocab)\n",
    "\n",
    "vocab_size = size + size2\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "num_classes = 7\n",
    "embedding_dim = 64\n",
    "max_sequence_length = 1000000\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "hidden_dim = 256\n",
    "\n",
    "# Instantiate data loader and load/preprocess data\n",
    "tokenizer = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "test = FriendsDataset('sets/test_set1.json', tokenizer)\n",
    "X_test, y_test = test.preprocess_data(max_sequence_length)\n",
    "\n",
    "train = FriendsDataset('sets/train_set1.json', tokenizer)\n",
    "X_train, y_train = train.preprocess_data(max_sequence_length)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=batch_size)\n",
    "\n",
    "# Instantiate the speaker classifier model\n",
    "model = Classifier(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for utterances, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        utterances = utterances.long()\n",
    "        outputs = model(utterances)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for utterances, labels in test_loader:\n",
    "            utterances = utterances.long()\n",
    "            outputs = model(utterances)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch +1 }], Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nkproj",
   "language": "python",
   "name": "nkproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
