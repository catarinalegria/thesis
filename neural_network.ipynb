{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 15:06:11.057913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "#from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.out = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self,ids,mask,token_type_ids):\n",
    "        _,o2= self.bert_model(ids,attention_mask=mask,token_type_ids=token_type_ids, return_dict=False)\n",
    "        \n",
    "        out= self.out(o2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FriendsDataset(Dataset):\n",
    "    def __init__(self, data_file, tokenizer):\n",
    "        self.data_file = data_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.utterances, self.labels = self.load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.utterances[idx], self.labels[idx]\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.data_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        utterances = []\n",
    "        labels = []\n",
    "        for utterance in data:\n",
    "            utterances.append(utterance['transcript'])\n",
    "\n",
    "            if utterance['speakers'][0] not in  ['Monica Geller', 'Joey Tribbiani', 'Chandler Bing', 'Phoebe Buffay', 'Ross Geller', 'Rachel Green']:\n",
    "                labels.append(\"Other\")\n",
    "            else:\n",
    "                labels.append(utterance['speakers'][0])\n",
    "\n",
    "        self.encoder.fit(labels)\n",
    "        labels = self.encoder.transform(labels)\n",
    "\n",
    "        return utterances, labels\n",
    "\n",
    "    def preprocess_data(self, max_sequence_length):\n",
    "        tokenized_utterances = [self.tokenizer.encode(utterance)[:max_sequence_length] for utterance in self.utterances]\n",
    "        padded_utterances = []\n",
    "        for tokenized_utterance in tokenized_utterances:\n",
    "            padded_utterance = list(tokenized_utterance)\n",
    "            if len(padded_utterance) < max_sequence_length:\n",
    "                padded_utterance.extend([0] * (max_sequence_length - len(padded_utterance)))\n",
    "            padded_utterances.append(padded_utterance)\n",
    "\n",
    "        labels = torch.tensor(self.labels)\n",
    "\n",
    "        return padded_utterances, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, num_classes, embedding_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.sentence_transformer.encode(x)\n",
    "        return self.fc(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m X_test, y_test \u001b[39m=\u001b[39m test\u001b[39m.\u001b[39mpreprocess_data(max_sequence_length)\n\u001b[1;32m     14\u001b[0m train \u001b[39m=\u001b[39m FriendsDataset(\u001b[39m'\u001b[39m\u001b[39msets/train_set1.json\u001b[39m\u001b[39m'\u001b[39m, tokenizer)\n\u001b[0;32m---> 15\u001b[0m X_train, y_train \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39;49mpreprocess_data(max_sequence_length)\n\u001b[1;32m     17\u001b[0m \u001b[39m# Convert data to PyTorch tensors\u001b[39;00m\n\u001b[1;32m     18\u001b[0m X_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(X_train)\n",
      "Cell \u001b[0;32mIn[19], line 34\u001b[0m, in \u001b[0;36mFriendsDataset.preprocess_data\u001b[0;34m(self, max_sequence_length)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_data\u001b[39m(\u001b[39mself\u001b[39m, max_sequence_length):\n\u001b[0;32m---> 34\u001b[0m     tokenized_utterances \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode(utterance)[:max_sequence_length] \u001b[39mfor\u001b[39;00m utterance \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mutterances]\n\u001b[1;32m     35\u001b[0m     padded_utterances \u001b[39m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m     \u001b[39mfor\u001b[39;00m tokenized_utterance \u001b[39min\u001b[39;00m tokenized_utterances:\n",
      "Cell \u001b[0;32mIn[19], line 34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_data\u001b[39m(\u001b[39mself\u001b[39m, max_sequence_length):\n\u001b[0;32m---> 34\u001b[0m     tokenized_utterances \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mencode(utterance)[:max_sequence_length] \u001b[39mfor\u001b[39;00m utterance \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mutterances]\n\u001b[1;32m     35\u001b[0m     padded_utterances \u001b[39m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m     \u001b[39mfor\u001b[39;00m tokenized_utterance \u001b[39min\u001b[39;00m tokenized_utterances:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:134\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, sentences: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]],\n\u001b[1;32m    112\u001b[0m            batch_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m,\n\u001b[1;32m    113\u001b[0m            show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m            device: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    118\u001b[0m            normalize_embeddings: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[List[Tensor], ndarray, Tensor]:\n\u001b[1;32m    119\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39m    Computes sentence embeddings\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m       By default, a list of tensors is returned. If convert_to_tensor, a stacked tensor is returned. If convert_to_numpy, a numpy matrix is returned.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval()\n\u001b[1;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m show_progress_bar \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         show_progress_bar \u001b[39m=\u001b[39m (logger\u001b[39m.\u001b[39mgetEffectiveLevel()\u001b[39m==\u001b[39mlogging\u001b[39m.\u001b[39mINFO \u001b[39mor\u001b[39;00m logger\u001b[39m.\u001b[39mgetEffectiveLevel()\u001b[39m==\u001b[39mlogging\u001b[39m.\u001b[39mDEBUG)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2307\u001b[0m, in \u001b[0;36mModule.eval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval\u001b[39m(\u001b[39mself\u001b[39m: T) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m   2292\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sets the module in evaluation mode.\u001b[39;00m\n\u001b[1;32m   2293\u001b[0m \n\u001b[1;32m   2294\u001b[0m \u001b[39m    This has any effect only on certain modules. See documentations of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2305\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m   2306\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2288\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m mode\n\u001b[1;32m   2287\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m-> 2288\u001b[0m     module\u001b[39m.\u001b[39;49mtrain(mode)\n\u001b[1;32m   2289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2288\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m mode\n\u001b[1;32m   2287\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m-> 2288\u001b[0m     module\u001b[39m.\u001b[39;49mtrain(mode)\n\u001b[1;32m   2289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.train at line 2288 (5 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2288\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m mode\n\u001b[1;32m   2287\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m-> 2288\u001b[0m     module\u001b[39m.\u001b[39;49mtrain(mode)\n\u001b[1;32m   2289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2269\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2266\u001b[0m             \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   2267\u001b[0m                 \u001b[39myield\u001b[39;00m m\n\u001b[0;32m-> 2269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m: T, mode: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m   2270\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sets the module in training mode.\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \n\u001b[1;32m   2272\u001b[0m \u001b[39m    This has any effect only on certain modules. See documentations of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2282\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m   2283\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2284\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(mode, \u001b[39mbool\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "num_classes = 7\n",
    "embedding_dim = 768\n",
    "max_sequence_length = 10000\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Instantiate data loader and load/preprocess data\n",
    "tokenizer = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "test = FriendsDataset('sets/test_set1.json', tokenizer)\n",
    "X_test, y_test = test.preprocess_data(max_sequence_length)\n",
    "\n",
    "train = FriendsDataset('sets/train_set1.json', tokenizer)\n",
    "X_train, y_train = train.preprocess_data(max_sequence_length)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=batch_size)\n",
    "\n",
    "# Instantiate the speaker classifier model\n",
    "speaker_classifier = Classifier(num_classes, embedding_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(speaker_classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    speaker_classifier.train()\n",
    "    for utterances, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = speaker_classifier(utterances)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    speaker_classifier.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for utterances, labels in test_loader:\n",
    "            outputs = speaker_classifier(utterances)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nkproj",
   "language": "python",
   "name": "nkproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
